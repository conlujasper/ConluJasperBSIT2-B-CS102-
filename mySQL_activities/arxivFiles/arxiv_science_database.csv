"","id","title","author_id","description","content","date"
"1","1","Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization","Zhaolin Ren, Na Li","Machine Learning (cs.LG)","This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average.","Thu, 7 Mar 2024 18:58:26 UTC (677 KB)"
"2","2","SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions","Ilias Diakonikolas, Daniel Kane, Lisheng Ren, Yuxin Sun","Machine Learning (cs.LG)","We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts. In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite. While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. In this work, we establish that the latter condition is indeed not necessary. In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only. Our result naturally generalizes to the setting of a hidden subspace. Leveraging our general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of concrete estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees.","Thu, 7 Mar 2024 18:49:32 UTC (32 KB)"
"3","3","Tight general bounds for the extremal numbers of 0-1 matrices","Barnabás Janzer, Oliver Janzer, Van Magnan, Abhishek Methuku","Combinatorics (math.CO)","A zero-one matrix $M$ is said to contain another zero-one matrix $A$ if we can delete some rows and columns of $M$ and replace some $1$-entries with $0$-entries such that the resulting matrix is $A$. The extremal number of $A$, denoted $operatorname{ex}(n,A)$, is the maximum number of $1$-entries that an $n	imes n$ zero-one matrix can have without containing $A$. The systematic study of this function for various patterns $A$ goes back to the work of Füredi and Hajnal from 1992, and the field has many connections to other areas of mathematics and theoretical computer science. The problem has been particularly extensively studied for so-called acyclic matrices, but very little is known about the general case (that is, the case where $A$ is not necessarily acyclic). We prove the first asymptotically tight general result by showing that if $A$ has at most $t$ $1$-entries in every row, then $operatorname{ex}(n,A)leq n^{2-1/t+o(1)}$. This verifies a conjecture of Methuku and Tomon. Our result also provides the first tight general bound for the extremal number of vertex-ordered graphs with interval chromatic number $2$, generalizing a celebrated result of Füredi, and Alon, Krivelevich and Sudakov about the (unordered) extremal number of bipartite graphs with maximum degree $t$ in one of the vertex classes.","Thu, 7 Mar 2024 18:31:26 UTC (16 KB)"
"4","4","A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation","Ankit Pensia","Data Structures and Algorithms (cs.DS)","We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a emph{corrupted} set of samples from $mathcal{N}(mu,mathbf{I}_d)$, where the unknown mean $mu in mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $mathrm{poly}(k,log d, 1/epsilon)$ and runtime $d^2 mathrm{poly}(k,log d,1/epsilon)$, where $epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic ($Omega(d^2)$), which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in emph{subquadratic} time using $mathrm{poly}(k,log d,1/epsilon)$ samples. We also provide analogous results for robust sparse PCA. Our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by Valiant.","Thu, 7 Mar 2024 18:23:51 UTC (46 KB)"
"5","5","Testing an entropy estimator related to the dynamical state of galaxy clusters","J. M. Zúniga, C. A. Caretta, A. P. González, E. García-Manzanárez","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","We propose the entropy estimator $H_Z$, calculated from global dynamical parameters, in an attempt to capture the degree of evolution of galaxy systems. We assume that the observed (spatial and velocity) distributions of member galaxies in these systems evolve over time towards states of higher dynamical relaxation (higher entropy), becoming more random and homogeneous in virial equilibrium. Thus, the $H_Z$-entropy should correspond to the gravitacional assembly state of the systems. This was tested in a sample of 70 well sampled clusters in the Local Universe whose gravitational assembly state, classified from optical and X-ray analysis of substructures, shows clear statistical correlation with $H_Z$. This estimator was also tested on a sample of clusters (halos) from the IllustrisTNG simulations, obtaining results in agreement with the observational ones.","Thu, 7 Mar 2024 18:20:24 UTC (3,220 KB)"
"6","6","On $[1,2]$-Domination in Interval and Circle Graphs","Mohsen Alambardar Meybodi, Abolfazl Poureidi","Computational Complexity (cs.CC)","A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)setminus S$ is adjacent to at least one vertex in $S$. Chellali et al. in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set. A set $D subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs. This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs. Although for $jgeq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding. In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique. Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete.","Thu, 7 Mar 2024 17:43:21 UTC (523 KB)"
"7","7","Closed-loop Performance Optimization of Model Predictive Control with Robustness Guarantees","Riccardo Zuliani, Efe C. Balta, John Lygeros","Systems and Control (eess.SY)","Model mismatch and process noise are two frequently occurring phenomena that can drastically affect the performance of model predictive control (MPC) in practical applications. We propose a principled way to tune the cost function and the constraints of linear MPC schemes to achieve good performance and robust constraint satisfaction on uncertain nonlinear dynamics with additive noise. The tuning is performed using a novel MPC tuning algorithm based on backpropagation developed in our earlier work. Using the scenario approach, we provide probabilistic bounds on the likelihood of closed-loop constraint violation over a finite horizon. We showcase the effectiveness of the proposed method on linear and nonlinear simulation examples.","Thu, 7 Mar 2024 16:58:00 UTC (84 KB)"
"8","8","The interdefinability of expansions of Belnap-Dunn logic","C. A. Middelburg","Logic in Computer Science (cs.LO)","Belnap-Dunn logic, also knows as the logic of First-Degree Entailment, is a logic that can serve as the underlying logic of theories that are inconsistent or incomplete. For various reasons, different expansions of Belnap-Dunn logic with non-classical connectives have been studied. This paper investigates the question whether those expansions are interdefinable with an expansion whose connectives include only classical connectives. This is worth knowing because it is difficult to say how close a logic with non-classical connectives is related to classical logic. The notion of interdefinability of logics used is based on a general notion of definability of a connective in a logic that seems to have been forgotten.","Thu, 7 Mar 2024 16:34:02 UTC (29 KB)"
"9","9","Implementation of soft-constrained MPC for Tracking using its semi-banded problem structure","Victor Gracia, Pablo Krupa, Daniel Limon, Teodoro Alamo","Systems and Control (eess.SY)","Model Predictive Control (MPC) is a popular control approach due to its ability to consider constraints, including input and state restrictions, while minimizing a cost function. However, in practice, said constraints can result in feasibility issues, either because the system model is not accurate or due to the existence of external disturbances. To mitigate this problem, a solution adopted by the MPC community is the use of soft constraints. In this article, we consider a not-so-typical methodology to encode soft constraints in a particular MPC formulation known as MPC for Tracking (MPCT), which has several advantages when compared to standard MPC formulations. The motivation behind the proposed encoding is to maintain the semi-banded structure of the ingredients of a recently proposed solver for the considered MPCT formulation, thus providing an efficient and fast solver when compared to alternative approaches from the literature. We show numerical results highlighting the benefits of the formulation and the computational efficiency of the solver.","Thu, 7 Mar 2024 15:50:09 UTC (166 KB)"
"10","10","Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic?","Dibyayan Chakraborty, Antoine Dailly, Florent Foucaud, Ralf Klasing","Data Structures and Algorithms (cs.DS)","In this paper, we study a dynamic analogue of the Path Cover problem, which can be solved in polynomial-time in directed acyclic graphs. A temporal digraph has an arc set that changes over discrete time-steps, if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal DAG. A temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path. Two temporal paths are temporally disjoint if they do not occupy any vertex at the same time. A temporal (resp. temporally disjoint) path cover is a collection of (resp. temporally disjoint) temporal paths that covers all vertices. In this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as Temporal Path Cover (TPC) and Temporally Disjoint Path Cover (TD-PC). We show that both problems are NP-hard even when the underlying DAG is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps. Moreover, TD-PC remains NP-hard even on temporal oriented trees. In contrast, we show that TPC is polynomial-time solvable on temporal oriented trees by a reduction to Clique Cover for (static undirected) weakly chordal graphs (a subclass of perfect graphs for which Clique Cover admits an efficient algorithm). This highlights an interesting algorithmic difference between the two problems. Although it is NP-hard on temporal oriented trees, TD-PC becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees. We also show that TPC (resp. TD-PC) admits an XP (resp. FPT) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected graph.","Thu, 7 Mar 2024 15:35:36 UTC (34 KB)"
"11","11","Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor","Songtao Tian, Zixiong Yu","Machine Learning (cs.LG)","Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications. In this paper, we identify a suitable scaling factor (denoted by $alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability. We show that if $alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity. We also highlight a surprising phenomenon: even if we allow $alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur. However, when $alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK. Our simulation studies on synthetic data and real classification tasks such as MNIST, CIFAR10 and CIFAR100 support our theoretical criteria for choosing $alpha$.","Thu, 7 Mar 2024 14:40:53 UTC (174 KB)"
"12","12","Scalable approximation and solvers for ionic electrodiffusion in cellular geometries","Pietro Benedusi, Ada J. Ellingsrud, Halvor Herlyng, Marie E. Rognes","Numerical Analysis (math.NA)","The activity and dynamics of excitable cells are fundamentally regulated and moderated by extracellular and intracellular ion concentrations and their electric potentials. The increasing availability of dense reconstructions of excitable tissue at extreme geometric detail pose a new and clear scientific computing challenge for computational modelling of ion dynamics and transport. In this paper, we design, develop and evaluate a scalable numerical algorithm for solving the time-dependent and nonlinear KNP-EMI equations describing ionic electrodiffusion for excitable cells with an explicit geometric representation of intracellular and extracellular compartments and interior interfaces. We also introduce and specify a set of model scenarios of increasing complexity suitable for benchmarking. Our solution strategy is based on an implicit-explicit discretization and linearization in time, a mixed finite element discretization of ion concentrations and electric potentials in intracellular and extracellular domains, and an algebraic multigrid-based, inexact block-diagonal preconditioner for GMRES. Numerical experiments with up to $10^8$ unknowns per time step and up to 256 cores demonstrate that this solution strategy is robust and scalable with respect to the problem size, time discretization and number of cores.","Thu, 7 Mar 2024 13:48:36 UTC (5,206 KB)"
"13","13","Optimal Denial-of-Service Attacks Against Status Updating","Saad Kriouile, Mohamad Assaad, Deniz Gündüz, Touraj Soleymani","Information Theory (cs.IT)","In this paper, we investigate denial-of-service attacks against status updating. The target system is modeled by a Markov chain and an unreliable wireless channel, and the performance of status updating in the target system is measured based on two metrics: age of information and age of incorrect information. Our objective is to devise optimal attack policies that strike a balance between the deterioration of the system's performance and the adversary's energy. We model the optimal problem as a Markov decision process and prove rigorously that the optimal jamming policy is a threshold-based policy under both metrics. In addition, we provide a low-complexity algorithm to obtain the optimal threshold value of the jamming policy. Our numerical results show that the networked system with the age-of-incorrect-information metric is less sensitive to jamming attacks than with the age-of-information metric. Index Terms-age of incorrect information, age of information, cyber-physical systems, status updating, remote monitoring.","Thu, 7 Mar 2024 13:44:01 UTC (261 KB)"
"14","14","On the stability and shadowing of tree-shifts of finite type","Dawid Bucki","Dynamical Systems (math.DS)","We investigate relations between the pseudo-orbit-tracing property, topological stability and openness for tree-shifts. We prove that a tree-shift is of finite type if and only if it has the pseudo-orbit-tracing property which implies that the tree-shift is topologically stable and all shift maps are open. We also present an example of a tree-shift for which all shift maps are open but which is not of finite type. It also turns out, that if a topologically stable tree-shift does not have isolated points then it is of finite type.","Thu, 7 Mar 2024 12:51:39 UTC (13 KB)"
"15","15","Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computationa","Atiyah Elsheikh","Computation and Language (cs.CL)","The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application this http URL the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SHCS.","Thu, 7 Mar 2024 11:30:56 UTC (21 KB)"
"16","16","LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression","Laurent Condat, Artavazd Maranjyan, Peter Richtárik","Optimization and Control (math.OC)","In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.","Thu, 7 Mar 2024 09:22:50 UTC (978 KB)"
"17","17","A mechanism-informed reinforcement learning framework for shape optimization of airfoils","Jingfeng Wang, Guanghui Hu","Numerical Analysis (math.NA)","In this study, we present the mechanism-informed reinforcement learning framework for airfoil shape optimization. By leveraging the twin delayed deep deterministic policy gradient algorithm for its notable stability, our approach addresses the complexities of optimizing shapes governed by fluid dynamics. The PDEs-based solver is adopted for its accuracy even when the configurations and geometries are extraordinarily changed during the exploration. Dual-weighted residual-based mesh refinement strategy is applied to ensure the accurate calculation of target functionals. To streamline the iterative optimization process and handle geometric deformations, our approach integrates Laplacian smoothing, adaptive refinement, and a Bézier fitting strategy. This combination not only remits mesh tangling but also guarantees a precise manipulation of the airfoil geometry. Our neural network architecture leverages Bézier curves for efficient dimensionality reduction, thereby enhancing the learning process and ensuring the geometric accuracy of the airfoil shapes. An attention mechanism is embedded within the network to calculate potential action on the state as well. Furthermore, we have introduced different reward and penalty mechanisms tailored to the specific challenges of airfoil optimization. This algorithm is designed to support the optimization task, facilitating a more targeted and effective approach for airfoil shape optimization.","Thu, 7 Mar 2024 08:48:42 UTC (1,185 KB)"
"18","18","Memetic Differential Evolution Methods for Semi-Supervised Clustering","Pierluigi Mansueto, Fabio Schoen","Optimization and Control (math.OC)","In this paper, we deal with semi-supervised Minimum Sum-of-Squares Clustering (MSSC) problems where background knowledge is given in the form of instance-level constraints. In particular, we take into account ""must-link"" and ""cannot-link"" constraints, each of which indicates if two dataset points should be associated to the same or to a different cluster. The presence of such constraints makes the problem at least as hard as its unsupervised version: it is no more true that each point is associated to its nearest cluster center, thus requiring some modifications in crucial operations, such as the assignment step. In this scenario, we propose a novel memetic strategy based on the Differential Evolution paradigm, directly extending a state-of-the-art framework recently proposed in the unsupervised clustering literature. As far as we know, our contribution represents the first attempt to define a memetic methodology designed to generate a (hopefully) optimal feasible solution for the semi-supervised MSSC problem. The proposal is compared with some state-of-the-art algorithms from the literature on a set of well-known datasets, highlighting its effectiveness and efficiency in finding good quality clustering solutions.","Thu, 7 Mar 2024 08:37:36 UTC (4,182 KB)"
"19","19","Switching Classes: Characterization and Computation","Dhanyamol Antony, Yixin Cao, Sagartanu Pal, R.B. Sandeep","Data Structures and Algorithms (cs.DS)","In a graph, the switching operation reverses adjacencies between a subset of vertices and the others. For a hereditary graph class $mathcal{G}$, we are concerned with the maximum subclass and the minimum superclass of $mathcal{G}$ that are closed under switching. We characterize the maximum subclass for many important classes $mathcal{G}$, and prove that it is finite when $mathcal{G}$ is minor-closed and omits at least one graph. For several graph classes, we develop polynomial-time algorithms to recognize the minimum superclass. We also show that the recognition of the superclass is NP-complete for $H$-free graphs when $H$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the Exponential Time Hypothesis.","Thu, 7 Mar 2024 06:55:42 UTC (52 KB)"
"20","20","Decentralized and Equitable Optimal Transport","Ivan Lau, Shiqian Ma, César A. Uribe","Optimization and Control (math.OC)","This paper considers the decentralized (discrete) optimal transport (D-OT) problem. In this setting, a network of agents seeks to design a transportation plan jointly, where the cost function is the sum of privately held costs for each agent. We reformulate the D-OT problem as a constraint-coupled optimization problem and propose a single-loop decentralized algorithm with an iteration complexity of O(1/{epsilon}) that matches existing centralized first-order approaches. Moreover, we propose the decentralized equitable optimal transport (DE-OT) problem. In DE-OT, in addition to cooperatively designing a transportation plan that minimizes transportation costs, agents seek to ensure equity in their individual costs. The iteration complexity of the proposed method to solve DE-OT is also O(1/{epsilon}). This rate improves existing centralized algorithms, where the best iteration complexity obtained is O(1/{epsilon}^2).","Thu, 7 Mar 2024 06:47:45 UTC (877 KB)"
"21","21","Regularized DeepIV with Model Selection","Zihao Li, Hui Lan, Vasilis Syrgkanis, Mengdi Wang, Masatoshi Uehara","Machine Learning (cs.LG)","In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation. Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution. Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function. We further show that our method allows model selection procedures that can achieve the oracle rates in the misspecified regime. When extended to an iterative estimator, our method matches the current state-of-the-art convergence rate. Our method is a Tikhonov regularized variant of the popular DeepIV method with a non-parametric MLE first-stage estimator, and our results provide the first rigorous guarantees for this empirically used method, showcasing the importance of regularization which was absent from the original work.","Thu, 7 Mar 2024 05:38:56 UTC (187 KB)"
"22","22","GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models","Tolga Dimlioglu, Anna Choromanska","Machine Learning (cs.LG)","We study distributed training of deep learning models in time-constrained environments. We propose a new algorithm that periodically pulls workers towards the center variable computed as a weighted average of workers, where the weights are inversely proportional to the gradient norms of the workers such that recovering the flat regions in the optimization landscape is prioritized. We develop two asynchronous variants of the proposed algorithm that we call Model-level and Layer-level Gradient-based Weighted Averaging (resp. MGRAWA and LGRAWA), which differ in terms of the weighting scheme that is either done with respect to the entire model or is applied layer-wise. On the theoretical front, we prove the convergence guarantee for the proposed approach in both convex and non-convex settings. We then experimentally demonstrate that our algorithms outperform the competitor methods by achieving faster convergence and recovering better quality and flatter local optima. We also carry out an ablation study to analyze the scalability of the proposed algorithms in more crowded distributed training environments. Finally, we report that our approach requires less frequent communication and fewer distributed updates compared to the state-of-the-art baselines.","Thu, 7 Mar 2024 04:22:34 UTC (2,925 KB)"
"23","23","Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control","Sadegh Sadeghi Tabas, Vidya Samadi","Machine Learning (cs.LG)","Changes in demand, various hydrological inputs, and environmental stressors are among the issues that water managers and policymakers face on a regular basis. These concerns have sparked interest in applying different techniques to determine reservoir operation policy decisions. As the resolution of the analysis increases, it becomes more difficult to effectively represent a real-world system using traditional methods such as Dynamic Programming (DP) and Stochastic Dynamic Programming (SDP) for determining the best reservoir operation policy. One of the challenges is the ""curse of dimensionality,"" which means the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function. Deep Reinforcement Learning (DRL) is an intelligent approach to overcome the curses of stochastic optimization problems for reservoir operation policy decisions. To our knowledge, this study is the first attempt that examine various novel DRL continuous-action policy gradient methods (PGMs), including Deep Deterministic Policy Gradients (DDPG), Twin Delayed DDPG (TD3), and two different versions of Soft Actor-Critic (SAC18 and SAC19) for optimizing reservoir operation policy. In this study, multiple DRL techniques were implemented in order to find the optimal operation policy of Folsom Reservoir in California, USA. The reservoir system supplies agricultural, municipal, hydropower, and environmental flow demands and flood control operations to the City of Sacramento. Analysis suggests that the TD3 and SAC are robust to meet the Folsom Reservoir's demands and optimize reservoir operation policies.","Thu, 7 Mar 2024 03:55:56 UTC (2,842 KB)"
"24","24","Computing Representatives of Persistent Homology Generators with a Double Twist","Tuyen Pham, Hubert Wagner","Algebraic Topology (math.AT)","With the growing availability of efficient tools, persistent homology is becoming a useful methodology in a variety of applications. Significant work has been devoted to implementing tools for persistent homology diagrams; however, computing representative cycles corresponding to each point in the diagram can still be inefficient. To circumvent this problem, we extend the twist algorithm of Chen and Kerber. Our extension is based on a new technique we call saving, which supplements their existing killing technique. The resulting two-pass strategy can be realized using an existing matrix reduction implementation as a black-box and improves the efficiency of computing representatives of persistent homology generators. We prove the correctness of the new approach and experimentally show its performance.","Wed, 6 Mar 2024 23:08:06 UTC (248 KB)"
"25","25","Time-lapse full-waveform permeability inversion: a feasibility study","Ziyi Yin, Mathias Louboutin, Olav Møyner, Felix J. Herrmann","Geophysics (physics.geo-ph)","Time-lapse seismic monitoring necessitates integrated workflows that combine seismic and reservoir modeling to enhance reservoir property estimation. We present a feasibility study of an end-to-end inversion framework that directly inverts for permeability from prestack time-lapse seismic data. To assess the method's robustness, we design experiments focusing on its sensitivity to initial models and potential errors in modeling. Our study leverages the Compass model to simulate CO2 storage in saline aquifers, which is derived from well and seismic data from the North Sea, a candidate site for geological carbon storage.","Wed, 6 Mar 2024 22:27:31 UTC (13,997 KB)"
"26","26","Directional Smoothness and Gradient Methods: Convergence and Adaptivity","Aaron Mishkin, Ahmed Khaled, Yuanhao Wang, Aaron Defazio, Robert M. Gower","Machine Learning (cs.LG)","We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness.","Wed, 6 Mar 2024 22:24:05 UTC (2,526 KB)"
"27","27","Assigning Entities to Teams as a Hypergraph Discovery Problem","Guilherme Ferraz de Arruda, Wan He, Nasimeh Heydaribeni, Tara Javidi, Yamir Moreno, Tina Eliassi-Rad","Social and Information Networks (cs.SI)","We propose a team assignment algorithm based on a hypergraph approach focusing on resilience and diffusion optimization. Specifically, our method is based on optimizing the algebraic connectivity of the Laplacian matrix of an edge-dependent vertex-weighted hypergraph. We used constrained simulated annealing, where we constrained the effort agents can exert to perform a task and the minimum effort a task requires to be completed. We evaluated our methods in terms of the number of unsuccessful patches to drive our solution into the feasible region and the cost of patching. We showed that our formulation provides more robust solutions than the original data and the greedy approach. We hope that our methods motivate further research in applying hypergraphs to similar problems in different research areas and in exploring variations of our methods.","Wed, 6 Mar 2024 21:30:44 UTC (1,308 KB)"
"28","28","Chance-Constrained Control for Safe Spacecraft Autonomy: Convex Programming Approach","Kenshiro Oguri","Optimization and Control (math.OC)","This paper presents a robust path-planning framework for safe spacecraft autonomy under uncertainty and develops a computationally tractable formulation based on convex programming. We utilize chance-constrained control to formulate the problem. It provides a mathematical framework to solve for a sequence of control policies that minimizes a probabilistic cost under probabilistic constraints with a user-defined confidence level (e.g., safety with 99.9% confidence). The framework enables the planner to directly control state distributions under operational uncertainties while ensuring the vehicle safety. This paper rigorously formulates the safe autonomy problem, gathers and extends techniques in literature to accommodate key cost/constraint functions that often arise in spacecraft path planning, and develops a tractable solution method. The presented framework is demonstrated via two representative numerical examples: safe autonomous rendezvous and orbit maintenance in cislunar space, both under uncertainties due to navigation error from Kalman filter, execution error via Gates model, and imperfect force models.","Wed, 6 Mar 2024 21:27:31 UTC (2,694 KB)"
"29","29","Bridging Computational Notions of Depth","Laurent Bienvenu, Christopher P. Porter","Logic in Computer Science (cs.LO)","In this article, we study the relationship between notions of depth for sequences, namely, Bennett's notions of strong and weak depth, and deep $Pi^0_1$ classes, introduced by the authors and motivated by previous work of Levin. For the first main result of the study, we show that every member of a $Pi^0_1$ class is order-deep, a property that implies strong depth. From this result, we obtain new examples of strongly deep sequences based on properties studied in computability theory and algorithmic randomness. We further show that not every strongly deep sequence is a member of a deep $Pi^0_1$ class. For the second main result, we show that the collection of strongly deep sequences is negligible, which is equivalent to the statement that the probability of computing a strongly deep sequence with some random oracle is 0, a property also shared by every deep $Pi^0_1$ class. Finally, we show that variants of strong depth, given in terms of a priori complexity and monotone complexity, are equivalent to weak depth.","Wed, 6 Mar 2024 20:44:12 UTC (22 KB)"
"30","30","Length Functions and the Dimension of Points in Self-Similar Fractal Trees","Christopher P. Porter","Logic (math.LO)","In this paper, we study the effective dimension of points in infinite fractal trees generated recursively by a finite tree over some alphabet. Using unequal costs coding, we associate a length function with each such fractal tree and show that the channel capacity of the length function is equal to the similarity dimension of the fractal tree (up to a multiplicative constant determined by the size of the alphabet over which our tree is defined). Using this result, we derive formulas for calculating the effective dimension and strong effective dimension of points in fractal trees, establishing analogues of several results due to Lutz and Mayordomo, who studied the effective dimension of points in self-similar fractals in Euclidean space. Lastly, we explore the connections between the channel capacity of a length function derived from a finite tree and the measure of maximum entropy on a related directed multigraph that encodes the structure of our tree, drawing on work by Abram and Lagarias on path sets, where a path set is a generalization of the notion of a sofic shift.","Wed, 6 Mar 2024 20:43:44 UTC (30 KB)"
"31","31","Online Learning with Unknown Constraints","Karthik Sridharan, Seung Won Wilson Yoo","Machine Learning (cs.LG)","We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round. The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round. We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint. On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that captures the difficulty of safe learning. We complement our result with an asymptotic lower bound that shows that the aforementioned complexity measure is necessary. When the constraints are linear, we instantiate our result to provide a concrete algorithm with $sqrt{T}$ regret using a scaling transformation that balances optimistic exploration with pessimistic constraint satisfaction.","Wed, 6 Mar 2024 20:23:59 UTC (474 KB)"
"32","32","Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems","Wesley A. Suttle, Vipul K. Sharma, Krishna C. Kosaraju, S. Sivaranjani, Ji Liu, Vijay Gupta, Brian M. Sadler","Machine Learning (cs.LG)","We develop provably safe and convergent reinforcement learning (RL) algorithms for control of nonlinear dynamical systems, bridging the gap between the hard safety guarantees of control theory and the convergence guarantees of RL theory. Recent advances at the intersection of control and RL follow a two-stage, safety filter approach to enforcing hard safety constraints: model-free RL is used to learn a potentially unsafe controller, whose actions are projected onto safe sets prescribed, for example, by a control barrier function. Though safe, such approaches lose any convergence guarantees enjoyed by the underlying RL methods. In this paper, we develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment. We validate the efficacy of our approach in simulation, including safe control of a quadcopter in a challenging obstacle avoidance problem, and demonstrate that it outperforms existing benchmarks.","Wed, 6 Mar 2024 19:39:20 UTC (3,094 KB)"
"33","33","On Outer Bi-Lipschitz Extensions of Linear Johnson-Lindenstrauss Embeddings of Subsets of $mathbb{R}^N$","Rafael Chiclana, Mark A. Iwen, Mark Philip Roach","Metric Geometry (math.MG)","The celebrated Johnson-Lindenstrauss lemma states that for all $varepsilon in (0,1)$ and finite sets $X subseteq mathbb{R}^N$ with $n>1$ elements, there exists a matrix $Phi in mathbb{R}^{m 	imes N}$ with $m=mathcal{O}(varepsilon^{-2}log n)$ such that [ (1 - varepsilon) |x-y|_2 leq |Phi x-Phi y|_2 leq (1+varepsilon)| x- y|_2 quad forall, x, y in X.] Herein we consider terminal embedding results which have recently been introduced in the computer science literature as stronger extensions of the Johnson-Lindenstrauss lemma for finite sets. After a short survey of this relatively recent line of work, we extend the theory of terminal embeddings to hold for arbitrary (e.g., infinite) subsets $X subseteq mathbb{R}^N$, and then specialize our generalized results to the case where $X$ is a low-dimensional compact submanifold of $mathbb{R}^N$. In particular, we prove the following generalization of the Johnson-Lindenstrauss lemma: For all $varepsilon in (0,1)$ and $Xsubseteqmathbb{R}^N$, there exists a terminal embedding $f: mathbb{R}^N longrightarrow mathbb{R}^{m}$ such that $$(1 - varepsilon) | x - y |_2 leq left| f(x) - f(y) ight|_2 leq (1 + varepsilon) | x - y |_2 quad forall , x in X ~{m and}~ forall , y in mathbb{R}^N.$$ Crucially, we show that the dimension $m$ of the range of $f$ above is optimal up to multiplicative constants, satisfying $m=mathcal{O}(varepsilon^{-2} omega^2(S_X))$, where $omega(S_X)$ is the Gaussian width of the set of unit secants of $X$, $S_X=overline{{(x-y)/|x-y|_2 colon x 
eq y in X}}$. Furthermore, our proofs are constructive and yield algorithms for computing a general class of terminal embeddings $f$, an instance of which is demonstrated herein to allow for more accurate compressive nearest neighbor classification than standard linear Johnson-Lindenstrauss embeddings do in practice.","Wed, 6 Mar 2024 18:04:45 UTC (1,133 KB)"
"34","34","A Categorical Treatment of Open Linear Systems","Dario Stein, Richard Samuelson","Logic in Computer Science (cs.LO)","An open stochastic system à la Willems is a system affected two qualitatively different kinds of uncertainty -- one is probabilistic fluctuation, and the other one is nondeterminism caused by lack of information. We give a formalization of open stochastic systems in the language of category theory. A new construction, which we term copartiality, is needed to model the propagating lack of information (which corresponds to varying sigma-algebras). As a concrete example, we discuss extended Gaussian distributions, which combine Gaussian probability with nondeterminism and correspond precisely to Willems' notion of Gaussian linear systems. We describe them both as measure-theoretic and abstract categorical entities, which enables us to rigorously describe a variety of phenomena like noisy physical laws and uninformative priors in Bayesian statistics. The category of extended Gaussian maps can be seen as a mutual generalization of Gaussian probability and linear relations, which connects the literature on categorical probability with ideas from control theory like signal-flow diagrams.","Wed, 6 Mar 2024 18:42:06 UTC (659 KB)"
"35","35","Polynomial Calculus sizes over the Boolean and Fourier bases are incomparable","Sasank Mouli","Computational Complexity (cs.CC)","For every $n >0$, we show the existence of a CNF tautology over $O(n^2)$ variables of width $O(log n)$ such that it has a Polynomial Calculus Resolution refutation over ${0,1}$ variables of size $O(n^3polylog(n))$ but any Polynomial Calculus refutation over ${+1,-1}$ variables requires size $2^{Omega(n)}$. This shows that Polynomial Calculus sizes over the ${0,1}$ and ${+1,-1}$ bases are incomparable (since Tseitin tautologies show a separation in the other direction) and answers an open problem posed by Sokolov [Sok20] and Razborov.","Wed, 6 Mar 2024 18:42:01 UTC (18 KB)"
"36","36","Risk-Sensitive Mean Field Games with Common Noise: A Theoretical Study with Applications to Interbank Markets","Xin Yue Ren, Dena Firoozi","Optimization and Control (math.OC)","In this paper, we address linear-quadratic-Gaussian (LQG) risk-sensitive mean field games (MFGs) with common noise. In this framework agents are exposed to a common noise and aim to minimize an exponential cost functional that reflects their risk sensitivity. We leverage the convex analysis method to derive the optimal strategies of agents in the limit as the number of agents goes to infinity. These strategies yield a Nash equilibrium for the limiting model. The model is then applied to interbank markets, focusing on optimizing lending and borrowing activities to assess systemic and individual bank risks when reserves drop below a critical threshold. We employ Fokker-Planck equations and the first hitting time method to formulate the overall probability of a bank or market default. We observe that the risk-averse behavior of agents reduces the probability of individual defaults and systemic risk, enhancing the resilience of the financial system. Adopting a similar approach based on stochastic Fokker-Planck equations, we further expand our analysis to investigate the conditional probabilities of individual default under specific trajectories of the common market shock.","Wed, 6 Mar 2024 18:27:18 UTC (1,756 KB)"
"37","37","Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications","Arun Jambulapati, Syamantak Kumar, Jerry Li, Shourya Pandey, Ankit Pensia, Kevin Tian","Numerical Analysis (math.NA)","The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood. Our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-PCA. For a quadratic form notion of approximation we term ePCA (energy PCA), we show deflation methods suffer no parameter loss. For an alternative well-studied approximation notion we term cPCA (correlation PCA), we tightly characterize the parameter regimes where deflation methods are feasible. Moreover, we show that in all feasible regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA algorithms robust to dataset contamination, improving prior work both in sample complexity and approximation quality.","Wed, 6 Mar 2024 18:07:20 UTC (91 KB)[v2] Thu, 7 Mar 2024 02:12:55 UTC (91 KB)"
"38","38","Public-data Assisted Private Stochastic Optimization: Power and Limitations","Enayat Ullah, Michael Menart, Raef Bassily, Cristóbal Guzmán, Raman Arora","Machine Learning (cs.LG)","We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(epsilon,delta)$-PA-DP has excess risk $	ilde{Omega}ig(minig{frac{1}{sqrt{n_{	ext{pub}}}},frac{1}{sqrt{n}}+frac{sqrt{d}}{nepsilon} ig} ig)$, where $d$ is the dimension, ${n_{	ext{pub}}}$ is the number of public samples, ${n_{	ext{priv}}}$ is the number of private samples, and $n={n_{	ext{pub}}}+{n_{	ext{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with 	extit{unlabeled} public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $	ilde{O}({n_{	ext{priv}}}epsilon)$ unlabeled public samples, achieves the dimension independent rate $	ilde{O}ig(frac{1}{sqrt{n_{	ext{priv}}}} + frac{1}{sqrt{n_{	ext{priv}}epsilon}}ig)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite fat-shattering dimension with applications to neural networks and non-Euclidean geometries.","Wed, 6 Mar 2024 17:06:11 UTC (48 KB)"
"39","39","Accelerating Convergence of Score-Based Diffusion Models, Provably","Gen Li, Yu Huang, Timofey Efimov, Yuting Wei, Yuejie Chi, Yuxin Chen","Machine Learning (cs.LG)","Score-based diffusion models, while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up diffusion generative modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory accommodates $ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution.","Wed, 6 Mar 2024 17:02:39 UTC (4,391 KB)"
"40","40","Linear and nonlinear system identification under $ell_1$- and group-Lasso regularization via L-BFGS-B","Alberto Bemporad","Systems and Control (eess.SY)","In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks. We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python implementation of the proposed identification method is available in the package 	exttt{jax-sysid}, available at url{this https URL}.","Wed, 6 Mar 2024 16:17:34 UTC (65 KB)"
"41","41","Realizability of Rectangular Euler Diagrams","Dominik Dürrschnabel, Uta Priss","Computational Geometry (cs.CG)","Euler diagrams are a tool for the graphical representation of set relations. Due to their simple way of visualizing elements in the sets by geometric containment, they are easily readable by an inexperienced reader. Euler diagrams where the sets are visualized as aligned rectangles are of special interest. In this work, we link the existence of such rectangular Euler diagrams to the order dimension of an associated order relation. For this, we consider Euler diagrams in one and two dimensions. In the one-dimensional case, this correspondence provides us with a polynomial-time algorithm to compute the Euler diagrams, while the two-dimensional case results in an exponential-time algorithm.","Wed, 6 Mar 2024 15:53:47 UTC (172 KB)"
"42","42","Largest common subgraph of two forests","Dieter Rautenbach, Florian Werner","Data Structures and Algorithms (cs.DS)","A common subgraph of two graphs $G_1$ and $G_2$ is a graph that is isomorphic to subgraphs of $G_1$ and $G_2$. In the largest common subgraph problem the task is to determine a common subgraph for two given graphs $G_1$ and $G_2$ that is of maximum possible size ${m lcs}(G_1,G_2)$. This natural problem generalizes the well-studied graph isomorphism problem, has many applications, and remains NP-hard even restricted to unions of paths. We present a simple $4$-approximation algorithm for forests, and, for every fixed $epsilonin (0,1)$, we show that, for two given forests $F_1$ and $F_2$ of order at most $n$, one can determine in polynomial time a common subgraph $F$ of $F_1$ and $F_2$ with at least ${m lcs}(F_1,F_2)-epsilon n$ edges. Restricted to instances with ${m lcs}(F_1,F_2)geq cn$ for some fixed positive $c$, this yields a polynomial time approximation scheme. Our approach relies on the approximation of the given forests by structurally simpler forests that are composed of copies of only $O(log (n))$ different starlike rooted trees and iterative quantizations of the options for the solutions.","Wed, 6 Mar 2024 13:25:41 UTC (18 KB)"
"43","43","Spectral Phase Transition and Optimal PCA in Block-Structured Spiked models","Pierre Mergny, Justin Ko, Florent Krzakala","Machine Learning (stat.ML)","We discuss the inhomogeneous spiked Wigner model, a theoretical framework recently introduced to study structured noise in various learning scenarios, through the prism of random matrix theory, with a specific focus on its spectral properties. Our primary objective is to find an optimal spectral method and to extend the celebrated cite{BBP} (BBP) phase transition criterion -- well-known in the homogeneous case -- to our inhomogeneous, block-structured, Wigner model. We provide a thorough rigorous analysis of a transformed matrix and show that the transition for the appearance of 1) an outlier outside the bulk of the limiting spectral distribution and 2) a positive overlap between the associated eigenvector and the signal, occurs precisely at the optimal threshold, making the proposed spectral method optimal within the class of iterative methods for the inhomogeneous Wigner problem.","Wed, 6 Mar 2024 13:23:55 UTC (74 KB)"
"44","44","On the Structure of Hamiltonian Graphs with Small Independence Number","Nikola Jedličková, Jan Kratochvíl","Combinatorics (math.CO)","A Hamiltonian path (cycle) in a graph is a path (cycle, respectively) which passes through all of its vertices. The problems of deciding the existence of a Hamiltonian cycle (path) in an input graph are well known to be NP-complete, and restricted classes of graphs which allow for their polynomial-time solutions are intensively investigated. Until very recently the complexity was open even for graphs of independence number at most 3. So far unpublished result of Jedličková and Kratochv'ıl [arXiv:2309.09228] shows that for every integer $k$, Hamiltonian path and cycle are polynomial-time solvable in graphs of independence number bounded by $k$. As a companion structural result, we determine explicit obstacles for the existence of a Hamiltonian path for small values of $k$, namely for graphs of independence number 2, 3, and 4. Identifying these obstacles in an input graph yields alternative polynomial-time algorithms for Hamiltonian path and cycle with no large hidden multiplicative constants.","Wed, 6 Mar 2024 12:39:44 UTC (629 KB)"
"45","45","Robust Safety-Critical Control for Systems with Sporadic Measurements and Dwell Time Constraints","Joseph Breeden, Luca Zaccarian, Dimitra Panagou","Systems and Control (eess.SY)","This paper presents extensions of control barrier function (CBF) theory to systems with disturbances wherein a controller only receives measurements infrequently and operates open-loop between measurements, while still satisfying state constraints. The paper considers both impulsive and continuous actuators, and models the actuators, measurements, disturbances, and timing constraints as a hybrid dynamical system. We then design an open-loop observer that bounds the worst-case uncertainty between measurements. We develop definitions of CBFs for both actuation cases, and corresponding conditions on the control input to guarantee satisfaction of the state constraints. We apply these conditions to simulations of a satellite rendezvous in an elliptical orbit and autonomous orbit stationkeeping.","Wed, 6 Mar 2024 12:33:15 UTC (390 KB)"
"46","46","Finite elements for Matérn-type random fields: Uncertainty in computational mechanics and design optimization","Tobias Duswald, Brendan Keith, Boyan Lazarov, Socratis Petrides, Barbara Wohlmuth","Computational Engineering, Finance, and Science (cs.CE)","This work highlights an approach for incorporating realistic uncertainties into scientific computing workflows based on finite elements, focusing on applications in computational mechanics and design optimization. We leverage Matérn-type Gaussian random fields (GRFs) generated using the SPDE method to model aleatoric uncertainties, including environmental influences, variating material properties, and geometric ambiguities. Our focus lies on delivering practical GRF realizations that accurately capture imperfections and variations and understanding how they impact the predictions of computational models and the topology of optimized designs. We describe a numerical algorithm based on solving a generalized SPDE to sample GRFs on arbitrary meshed domains. The algorithm leverages established techniques and integrates seamlessly with the open-source finite element library MFEM and associated scientific computing workflows, like those found in industrial and national laboratory settings. Our solver scales efficiently for large-scale problems and supports various domain types, including surfaces and embedded manifolds. We showcase its versatility through biomechanics and topology optimization applications. The flexibility and efficiency of SPDE-based GRF generation empower us to run large-scale optimization problems on 2D and 3D domains, including finding optimized designs on embedded surfaces, and to generate topologies beyond the reach of conventional techniques. Moreover, these capabilities allow us to model geometric uncertainties of reconstructed submanifolds, such as the surfaces of cerebral aneurysms. In addition to offering benefits in these specific domains, the proposed techniques transcend specific applications and generalize to arbitrary forward and backward problems in uncertainty quantification involving finite elements.","Wed, 6 Mar 2024 12:28:14 UTC (47,016 KB)"
"47","47","Exact objectives of random linear programs and mean widths of random polyhedrons","Mihailo Stojnic","Optimization and Control (math.OC)","We consider emph{random linear programs} (rlps) as a subclass of emph{random optimization problems} (rops) and study their typical behavior. Our particular focus is on appropriate linear objectives which connect the rlps to the mean widths of random polyhedrons/polytopes. Utilizing the powerful machinery of emph{random duality theory} (RDT) cite{StojnicRegRndDlt10}, we obtain, in a large dimensional context, the exact characterizations of the program's objectives. In particular, for any $alpha=lim_{nightarrowinfty}frac{m}{n}in(0,infty)$, any unit vector $mathbf{c}in{mathbb R}^n$, any fixed $mathbf{a}in{mathbb R}^n$, and $Ain {mathbb R}^{m	imes n}$ with iid standard normal entries, we have egin{eqnarray*} lim_{nightarrowinfty}{mathbb P}_{A} left ( (1-epsilon) xi_{opt}(alpha;mathbf{a}) leq min_{Amathbf{x}leq mathbf{a}}mathbf{c}^Tmathbf{x} leq (1+epsilon) xi_{opt}(alpha;mathbf{a}) ight ) longrightarrow 1, end{eqnarray*} where egin{equation*} xi_{opt}(alpha;mathbf{a}) 	riangleq min_{x>0} sqrt{x^2- x^2 lim_{nightarrowinfty} frac{sum_{i=1}^{m} left ( frac{1}{2} left (left ( frac{mathbf{a}_i}{x}ight )^2 + 1ight ) mbox{erfc}left( frac{mathbf{a}_i}{xsqrt{2}}ight ) - frac{mathbf{a}_i}{x} frac{e^{-frac{mathbf{a}_i^2}{2x^2}}}{sqrt{2pi}} ight ) }{n} }. end{equation*} For example, for $mathbf{a}=mathbf{1}$, one uncovers egin{equation*} xi_{opt}(alpha) = min_{x>0} sqrt{x^2- x^2 alpha left ( frac{1}{2} left ( frac{1}{x^2} + 1ight ) mbox{erfc} left ( frac{1}{xsqrt{2}}ight ) - frac{1}{x} frac{e^{-frac{1}{2x^2}}}{sqrt{2pi}} ight ) }. end{equation*} Moreover, $2 xi_{opt}(alpha)$ is precisely the concentrating point of the mean width of the polyhedron ${mathbf{x}|Amathbf{x} leq mathbf{1}}$.","Wed, 6 Mar 2024 11:51:52 UTC (152 KB)"
"48","48","A hybrid dynamical system approach to the impulsive control of spacecraft rendezvous (extended version)","Alexandre Seuret, Rafael Vazquez, Luca Zaccarian","Systems and Control (eess.SY)","This paper introduces a hybrid dynamical system methodology for managing impulsive control in spacecraft rendezvous and proximity operations under the Hill-Clohessy-Wiltshire model. We address the control design problem by isolating the out-of-plane from the in-plane dynamics and present a feedback control law for each of them. This law is based on a Lyapunov function tailored to each of the dynamics, capable of addressing thruster saturation and also a minimum impulse bit. These Lyapunov functions were found by reformulating the system's dynamics into coordinates that more intuitively represent their physical behavior. The effectiveness of our control laws is then shown through numerical simulation. This is an extended version of an ECC24 article of the same name, which includes the proofs omitted for lack of space.","Wed, 6 Mar 2024 11:42:35 UTC (1,160 KB)"
"49","49","Data-Driven Superstabilizing Control under Quadratically-Bounded Errors-in-Variables Noise","Jared Miller, Tianyu Dai, Mario Sznaier","Optimization and Control (math.OC)","The Error-in-Variables model of system identification/control involves nontrivial input and measurement corruption of observed data, resulting in generically nonconvex optimization problems. This paper performs full-state-feedback stabilizing control of all discrete-time linear systems that are consistent with observed data for which the input and measurement noise obey quadratic bounds. Instances of such quadratic bounds include elementwise norm bounds (at each time sample), energy bounds (across the entire signal), and chance constraints arising from (sub)gaussian noise. Superstabilizing controllers are generated through the solution of a sum-of-squares hierarchy of semidefinite programs. A theorem of alternatives is employed to eliminate the input and measurement noise process, thus improving tractability. Effectiveness of the scheme is generated on an example system in the chance-constrained set-membership setting where the input and state-measurement noise are i.i.d. normally distributed.","Wed, 6 Mar 2024 11:31:08 UTC (81 KB)"
"50","50","Medial Parametrization of Arbitrary Planar Compact Domains with Dipoles","Vinayak Krishnamurthy, Ergun Akleman","Graphics (cs.GR)","We present medial parametrization, a new approach to parameterizing any compact planar domain bounded by simple closed curves. The basic premise behind our proposed approach is to use two close Voronoi sites, which we call dipoles, to construct and reconstruct an approximate piecewise-linear version of the original boundary and medial axis through Voronoi tessellation. The boundaries and medial axes of such planar compact domains offer a natural way to describe the domain's interior. Any compact planar domain is homeomorphic to a compact unit circular disk admits a natural parameterization isomorphic to the polar parametrization of the disk. Specifically, the medial axis and the boundary generalize the radial and angular parameters, respectively. In this paper, we present a simple algorithm that puts these principles into practice. The algorithm is based on the simultaneous re-creation of the boundaries of the domain and its medial axis using Voronoi tessellation. This simultaneous re-creation provides partitions of the domain into a set of ""skinny"" convex polygons wherein each polygon is essentially a subset of the medial edges (which we call the spine) connected to the boundary through exactly two straight edges (which we call limbs). This unique structure enables us to convert the original Voronoi tessellation into quadrilaterals and triangles (at the poles of the medial axis) neatly ordered along the domain boundary, thereby allowing proper parametrization of the domain. Our approach is agnostic to the number of holes and disconnected components bounding the domain. We investigate the efficacy of our concept and algorithm through several examples.","Wed, 6 Mar 2024 11:28:47 UTC (7,980 KB)[v2] Thu, 7 Mar 2024 11:00:04 UTC (7,970 KB)"
"51","51","RouteExplainer: An Explanation Framework for Vehicle Routing Problem","Daisuke Kikuta, Hiroki Ikeuchi, Kengo Tajiri, Yuusuke Nakano","Machine Learning (cs.LG)","The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending counterfactual explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by Large Language Models (LLMs). We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining reasonable accuracy, thereby highlighting its potential for deployment in practical applications. Moreover, on the subject of a tourist route, we qualitatively evaluate explanations generated by our framework. This evaluation not only validates our framework but also shows the synergy between explanation frameworks and LLMs. See this https URL for our code, datasets, models, and demo.","Wed, 6 Mar 2024 10:01:35 UTC (9,937 KB)"
"52","52","Anisotropic power diagrams for polycrystal modelling: efficient generation of curved grains via optimal transport","Maciej Buze, Jean Feydy, Steven M. Roper, Karo Sedighiani, David P. Bourne","Materials Science (cond-mat.mtrl-sci)","The microstructure of metals and foams can be effectively modelled with anisotropic power diagrams (APDs), which provide control over the shape of individual grains. One major obstacle to the wider adoption of APDs is the computational cost that is associated with their generation. We propose a novel approach to generate APDs with prescribed statistical properties, including fine control over the size of individual grains. To this end, we rely on fast optimal transport algorithms that stream well on Graphics Processing Units (GPU) and handle non-uniform, anisotropic distance functions. This allows us to find APDs that best fit experimental data in (tens of) seconds, which unlocks their use for computational homogenisation. This is especially relevant to machine learning methods that require the generation of large collections of representative microstructures as training data. The paper is accompanied by a Python library, PyAPD, which is freely available at: this http URL.","Wed, 6 Mar 2024 09:30:31 UTC (10,184 KB)"
"53","53","Sparse convex relaxations in polynomial optimization","Gennadiy Averkov, Benjamin Peters, Sebastian Sager","Optimization and Control (math.OC)","We present a novel, general, and unifying point of view on sparse approaches to polynomial optimization. Solving polynomial optimization problems to global optimality is a ubiquitous challenge in many areas of science and engineering. Historically, different approaches on how to solve nonconvex polynomial optimization problems based on convex relaxations have been developed in different scientific communities. Here, we introduce the concept of monomial patterns. A pattern determines what monomials are to be linked by convex constraints in a convex relaxation of a polynomial optimization problem. This concept helps to understand existing approaches from different schools of thought, to develop novel relaxation schemes, and to derive a flexible duality theory, which can be specialized to many concrete situations that have been considered in the literature. We unify different approaches to polynomial optimization including polyhedral approximations, dense semidefinite relaxations, SONC, SAGE, and TSSOS in a self-contained exposition. We also carry out computational experiments to demonstrate the practical advantages of a flexible usage of pattern-based sparse relaxations of polynomial optimization problems.","Wed, 6 Mar 2024 09:09:40 UTC (1,540 KB)"
"54","54","DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training","Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, Jun Zhu","Machine Learning (cs.LG)","Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data. Code is available at url{this https URL}.","Wed, 6 Mar 2024 08:38:34 UTC (889 KB)[v2] Thu, 7 Mar 2024 04:53:51 UTC (884 KB)"
"55","55","TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs","Yanlai Chen, Yajie Ji, Akil Narayan, Zhenli Xu","Numerical Analysis (math.NA)","We introduce the Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of transport-dominated partial differential equations in an MOR-integrating PINNs framework. Building on the recent development of the GPT-PINN that is a network-of-networks design achieving snapshot-based model reduction, we design and test a novel paradigm for nonlinear model reduction that can effectively tackle problems with parameter-dependent discontinuities. Through incorporation of a shock-capturing loss function component as well as a parameter-dependent transform layer, the TGPT-PINN overcomes the limitations of linear model reduction in the transport-dominated regime. We demonstrate this new capability for nonlinear model reduction in the PINNs framework by several nontrivial parametric partial differential equations.","Wed, 6 Mar 2024 04:49:18 UTC (12,970 KB)"
"56","56","Robust Control Lyapunov-Value Functions for Nonlinear Disturbed Systems","Zheng Gong, Sylvia Herbert","Optimization and Control (math.OC)","Control Lyapunov Functions (CLFs) have been extensively used in the control community. A well-known drawback is the absence of a systematic way to construct CLFs for general nonlinear systems, and the problem can become more complex with input or state constraints. Our preliminary work on constructing Control Lyapunov Value Functions (CLVFs) using Hamilton-Jacobi (HJ) reachability analysis provides a method for finding a non-smooth CLF. In this paper, we extend our work on CLVFs to systems with bounded disturbance and define the Robust CLVF (R-CLVF). The R-CLVF naturally inherits all properties of the CLVF; i.e., it first identifies the ""smallest robust control invariant set (SRCIS)"" and stabilizes the system to it with a user-specified exponential rate. The region from which the exponential rate can be met is called the ""region of exponential stabilizability (ROES)."" We provide clearer definitions of the SRCIS and more rigorous proofs of several important theorems. Since the computation of the R-CLVF suffers from the ""curse of dimensionality,"" we also provide two techniques (warmstart and system decomposition) that solve it, along with necessary proofs. Three numerical examples are provided, validating our definition of SRCIS, illustrating the trade-off between a faster decay rate and a smaller ROES, and demonstrating the efficiency of computation using warmstart and decomposition.","Wed, 6 Mar 2024 04:45:06 UTC (515 KB)"
"57","57","Learning Constrained Optimization with Deep Augmented Lagrangian Methods","James Kotary, Ferdinando Fioretto","Machine Learning (cs.LG)","Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver. Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems. Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility. This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs. This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method. First it is shown that the poor convergence properties of classical Dual Ascent are reflected in poor convergence of the proposed training scheme. Then, by incorporating techniques from practical Augmented Lagrangian methods, we show how the training scheme can be improved to learn highly accurate constrained optimization solvers, for both convex and nonconvex problems.","Wed, 6 Mar 2024 04:43:22 UTC (679 KB)"
"58","58","Uncertainty quantification for deeponets with ensemble kalman inversion","Andrew Pensoneault, Xueyu Zhu","Machine Learning (cs.LG)","In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our innovative application of EKI enables us to efficiently train ensembles of DeepONets while obtaining informative uncertainty estimates for the output of interest. We deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating the computational demand due to large datasets during the training stage. Furthermore, we introduce a heuristic method to estimate the artificial dynamics covariance, thereby improving our uncertainty estimates. Finally, we demonstrate the effectiveness and versatility of our proposed methodology across various benchmark problems, showcasing its potential to address the pressing challenges of uncertainty quantification in DeepONets, especially for practical applications with limited and noisy data.","Wed, 6 Mar 2024 04:02:30 UTC (38,527 KB)"
"59","59","Secure Total Domination Number in Maximal Outerplanar Graphs","Yasufumi Aita, Toru Araki","Combinatorics (math.CO)","A subset $S$ of vertices in a graph $G$ is a secure total dominating set of $G$ if $S$ is a total dominating set of $G$ and, for each vertex $u 
otin S$, there is a vertex $v in S$ such that $uv$ is an edge and $(S setminus {v}) cup {u}$ is also a total dominating set of $G$. We show that if $G$ is a maximal outerplanar graph of order $n$, then $G$ has a total secure dominating set of size at most $lfloor 2n/3 floor$. Moreover, if an outerplanar graph $G$ of order $n$, then each secure total dominating set has at least $lceil (n+2)/3 ceil$ vertices. We show that these bounds are best possible.","Wed, 6 Mar 2024 02:00:22 UTC (71 KB)"
"60","60","am-AMM: An Auction-Managed Automated Market Maker","Austin Adams, Ciamac Moallemi, Sara Reynolds, Dan Robinson","Trading and Market Microstructure (q-fin.TR)","Automated market makers (AMMs) have emerged as the dominant market mechanism for trading on decentralized exchanges implemented on blockchains. This paper presents a single mechanism that targets two important unsolved problems for AMMs: reducing losses to informed orderflow, and maximizing revenue from uninformed orderflow. The ""auction-managed AMM"" works by running a censorship-resistant onchain auction for the right to temporarily act as ""pool manager"" for a constant-product AMM. The pool manager sets the swap fee rate on the pool, and also receives the accrued fees from swaps. The pool manager can exclusively capture some arbitrage by trading against the pool in response to small price movements, and also can set swap fees incorporating price sensitivity of retail orderflow and adapting to changing market conditions, with the benefits from both ultimately accruing to liquidity providers. Liquidity providers can enter and exit the pool freely in response to changing rent, though they must pay a small fee on withdrawal. We prove that under certain assumptions, this AMM should have higher liquidity in equilibrium than any standard, fixed-fee AMM.","Tue, 5 Mar 2024 23:28:50 UTC (27 KB)"
"61","61","Level Set Teleportation: An Optimization Perspective","Aaron Mishkin, Alberto Bietti, Robert M. Gower","Machine Learning (cs.LG)","We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function. Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee. For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates. To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products. We use this method to show that gradient methods with access to a teleportation oracle uniformly out-perform their standard versions on a variety of learning problems.","Tue, 5 Mar 2024 23:16:13 UTC (3,599 KB)"
"62","62","The vehicle routing problem with synchronization constraints and support vehicle-dependent service times","David Wittwer, Felix Tamke","Optimization and Control (math.OC)","Many production processes require the cooperation of various resources. Especially when using expensive machines, their utilization plays a decisive role in efficient production. In agricultural production or civil construction processes, e.g., harvesting or road building, the machines are typically mobile, and synchronization of different machine types is required to perform operations. In addition, the productivity of one type often depends on the availability of another type. In this paper, we consider two types of vehicles, called primary and support vehicles. Primary vehicles perform operations and are assisted by at least one support vehicle, with more support vehicles resulting in faster service times for primary vehicles. We call this practical problem the vehicle routing and scheduling problem with support vehicle-dependent service times and introduce two mixed-integer linear programming models. The first represents each support vehicle individually with binary decision variables, while the second considers the cumulative flow of support vehicles with integer decision variables. Furthermore, the models are defined on a graph that allows easy transformation into multiple variants. These variants are based on allowing or prohibiting switching support vehicles between primary vehicles and splitting services among primary vehicles. We show in our extensive computational experiments that: i) the integer representation of support vehicles is superior to the binary representation, ii) the benefit of additional vehicles is subject to saturation effects and depends on the ratio of support and primary vehicles, and iii) switching and splitting lead to problems that are more difficult to solve, but also result in better solutions with higher primary vehicle utilization.","Tue, 5 Mar 2024 22:46:35 UTC (767 KB)"
"63","63","Hypothesis Spaces for Deep Learning","Rui Wang, Yuesheng Xu, Mingsong Yan","Machine Learning (stat.ML)","This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of a finite number of kernel sessions determined by given data and the reproducing kernel.","Tue, 5 Mar 2024 22:42:29 UTC (27 KB)"
"64","64","Assortment Optimization For Conference Goodies With Indifferent Attendees","Fernanda Gutiérrez, Bernardo Subercaseaux","Optimization and Control (math.OC)","Conferences such as FUN with Algorithms routinely buy goodies (e.g., t-shirts, coffee mugs, etc) for their attendees. Often, said goodies come in different types, varying by color or design, and organizers need to decide how many goodies of each type to buy. We study the problem of buying optimal amounts of each type under a simple model of preferences by the attendees: they are indifferent to the types but want to be able to choose between more than one type of goodies at the time of their arrival. The indifference of attendees suggests that the optimal policy is to buy roughly equal amounts for every goodie type. Despite how intuitive this conjecture sounds, we show that this simple model of assortment optimization is quite rich, and even though we make progress towards proving the conjecture (e.g., we succeed when the number of goodie types is 2 or 3), the general case with K types remains open. We also present asymptotic results and computer simulations, and finally, to motivate further progress, we offer a reward of $100usd for a full proof.","Tue, 5 Mar 2024 21:22:39 UTC (691 KB)"
"65","65","From Noise to Signal: Unveiling Treatment Effects from Digital Health Data through Pharmacology-Informed Neural-SDE","Samira Pakravan, Nikolaos Evangelou, Maxime Usdin, Logan Brooks, James Lu","Quantitative Methods (q-bio.QM)","Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfactual simulation.","Tue, 5 Mar 2024 19:13:57 UTC (1,211 KB)"
"66","66","Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of Right","Tuan A. Bohoran, Polydoros N. Kampaktsis, Laura McLaughlin, Jay Leb, Gerry P. McCann, Archontis Giannakidis","Tissues and Organs (q-bio.TO)","The right ventricular (RV) function deterioration strongly predicts clinical outcomes in numerous circumstances. To boost the clinical deployment of ensemble regression methods that quantify RV volumes using tabular data from the widely available two-dimensional echocardiography (2DE), we propose to complement the volume predictions with uncertainty scores. To this end, we employ an instance-based method which uses the learned tree structure to identify the nearest training samples to a target instance and then uses a number of distribution types to more flexibly model the output. The probabilistic and point-prediction performances of the proposed framework are evaluated on a relatively small-scale dataset, comprising 100 end-diastolic and end-systolic RV volumes. The reference values for point performance were obtained from MRI. The results demonstrate that our flexible approach yields improved probabilistic and point performances over other state-of-the-art methods. The appropriateness of the proposed framework is showcased by providing exemplar cases. The estimated uncertainty embodies both aleatoric and epistemic types. This work aligns with trustworthy artificial intelligence since it can be used to enhance the decision-making process and reduce risks. The feature importance scores of our framework can be exploited to reduce the number of required 2DE views which could enhance the proposed pipeline's clinical application.","Mon, 4 Mar 2024 12:36:31 UTC (1,481 KB)"
"67","67","Finding Super-spreaders in Network Cascades","Elchanan Mossel, Anirudh Sridhar","Statistics Theory (math.ST)","Suppose that a cascade (e.g., an epidemic) spreads on an unknown graph, and only the infection times of vertices are observed. What can be learned about the graph from the infection times caused by multiple distinct cascades? Most of the literature on this topic focuses on the task of recovering the entire graph, which requires $Omega ( log n)$ cascades for an $n$-vertex bounded degree graph. Here we ask a different question: can the important parts of the graph be estimated from just a few (i.e., constant number) of cascades, even as $n$ grows large? In this work, we focus on identifying super-spreaders (i.e., high-degree vertices) from infection times caused by a Susceptible-Infected process on a graph. Our first main result shows that vertices of degree greater than $n^{3/4}$ can indeed be estimated from a constant number of cascades. Our algorithm for doing so leverages a novel connection between vertex degrees and the second derivative of the cumulative infection curve. Conversely, we show that estimating vertices of degree smaller than $n^{1/2}$ requires at least $log(n) / log log (n)$ cascades. Surprisingly, this matches (up to $log log n$ factors) the number of cascades needed to learn the emph{entire} graph if it is a tree.","Tue, 5 Mar 2024 18:43:45 UTC (970 KB)[v2] Wed, 6 Mar 2024 23:33:50 UTC (703 KB)"
"68","68","How Well Can Transformers Emulate In-context Newton's Method?","Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, Jason D. Lee","Machine Learning (cs.LG)","Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve $epsilon$ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent.","Tue, 5 Mar 2024 18:20:10 UTC (1,176 KB)"
"69","69","On the computation of stable coupled state-space models for dynamic substructuring applications","R.S.O. Dias, M. Martarelli, P. Chiariotti","Systems and Control (eess.SY)","This paper aims at introducing a methodology to compute stable coupled state-space models for dynamic substructuring applications by introducing two novel approaches targeted to accomplish this task: a) a procedure to impose Newtons's second law without relying on the use of undamped RCMs (residual compensation modes) and b) a novel approach to impose stability on unstable coupled state-space models. The enforcement of stability is performed by dividing the unstable model into two different models, one composed by the stable poles (stable model) and the other composed by the unstable ones (unstable model). Then, the poles of the unstable state-space model are forced to be stable, leading to the computation of a stabilized state-space model. Afterwards, to make sure that the Frequency Response Functions (FRFs) of the stabilized model well match the FRFs of the unstable model, the Least-Squares Frequency Domain (LSFD) method is exploited to update the modal parameters of the stabilized model composed by the pairs of complex conjugate poles. The validity of the proposed methodologies is presented and discussed by exploiting experimental data. Indeed, by exploiting the FRFs of a real system, accurate state-space models respecting Newton's second law are computed. Then, decoupling and coupling operations are performed with the identified state-space models, no matter the models resultant from the decoupling/coupling operations are unstable. Stability is then imposed on the computed unstable coupled model by following the approach proposed in this paper. The methodology proved to work well on these data. Moreover, the paper also shows that the coupled state-space models obtained using this methodology are suitable to be exploited in time-domain analyses and simulations.","Tue, 5 Mar 2024 18:19:55 UTC (4,127 KB)"
"70","70","Shuffling Momentum Gradient Algorithm for Convex Optimization","Trang H. Tran, Quoc Tran-Dinh, Lam M. Nguyen","Optimization and Control (math.OC)","The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs. Our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature.","Tue, 5 Mar 2024 18:19:02 UTC (395 KB)"
"71","71","Learning Explicitly Conditioned Sparsifying Transforms","Andrei Pătraşcu, Cristian Rusu, Paul Irofti","Numerical Analysis (math.NA)","Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains. Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers. Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model. Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms. We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art.","Tue, 5 Mar 2024 18:03:51 UTC (433 KB)"
"72","72","Low-rank approximated Kalman-Bucy filters using Oja's principal component flow for linear time-invariant systems","Daiki Tsuzuki, Kentaro Ohki","Optimization and Control (math.OC)","The Kalman-Bucy filter is widely used in various applications. However, the filter becomes computationally complex under large-scale systems. To address this problem, a low-rank approximated Kalman-Bucy filter consisting of Oja's principal component flow and a low-dimensional Riccati differential equation was proposed. However, the estimation error was established only for linear time-invariant systems with a symmetric system matrix. This study removes restrictions on the symmetricity of the system matrix and reveals the equilibrium points of the Oja flow and their stability for general real square matrices. In addition, the attraction domain for a set of stable equilibrium points is estimated. Based on these results, we demonstrate that the low-rank approximated Kalman-Bucy filter has a bounded estimation error covariance matrix when the system is controllable and observable.","Tue, 5 Mar 2024 16:45:41 UTC (72 KB)"
"73","73","A 2-categorical analysis of context comprehension","Greta Coraglia, Jacopo Emmenegger","Category Theory (math.CT)","We consider the equivalence between the two main categorical models for the type-theoretical operation of context comprehension, namely P. Dybjer's categories with families and B. Jacobs' comprehension categories, and generalise it to the non-discrete case. The classical equivalence can be summarised in the slogan: ""terms as sections"". By recognising ""terms as coalgebras"", we show how to use the structure-semantics adjunction to prove that a 2-category of comprehension categories is biequivalent to a 2-category of (non-discrete) categories with families. The biequivalence restricts to the classical one proved by Hofmann in the discrete case. It also provides a framework where to compare different morphisms of these structures that have appeared in the literature, varying on the degree of preservation of the relevant structure. We consider in particular morphisms defined by Claraimbault-Dybjer, Jacobs, Larrea, and Uemura.","Tue, 5 Mar 2024 16:11:51 UTC (87 KB)"
"74","74","Unifying Controller Design for Stabilizing Nonlinear Systems with Norm-Bounded Control Inputs","Ming Li, Zhiyong Sun, Siep Weiland","Systems and Control (eess.SY)","This paper revisits a classical challenge in the design of stabilizing controllers for nonlinear systems with a norm-bounded input constraint. By extending Lin-Sontag's universal formula and introducing a generic (state-dependent) scaling term, a unifying controller design method is proposed. The incorporation of this generic scaling term gives a unified controller and enables the derivation of alternative universal formulas with various favorable properties, which makes it suitable for tailored control designs to meet specific requirements and provides versatility across different control scenarios. Additionally, we present a constructive approach to determine the optimal scaling term, leading to an explicit solution to an optimization problem, named optimization-based universal formula. The resulting controller ensures asymptotic stability, satisfies a norm-bounded input constraint, and optimizes a predefined cost function. Finally, the essential properties of the unified controllers are analyzed, including smoothness, continuity at the origin, stability margin, and inverse optimality. Simulations validate the approach, showcasing its effectiveness in addressing a challenging stabilizing control problem of a nonlinear system.","Tue, 5 Mar 2024 15:06:16 UTC (1,262 KB)"
"75","75","The clique chromatic number of sparse random graphs","Manuel Fernandez V, Lutz Warnke","Combinatorics (math.CO)","The clique chromatic number of a graph is the smallest number of colors in a vertex coloring so that no maximal clique is monochromatic. In this paper, we determine the order of magnitude of the clique chromatic number of the random graph G_{n,p} for most edge-probabilities p in the range n^{-2/5} ll p ll 1. This resolves open problems and questions of Lichev, Mitsche and Warnke as well as Alon and Krievelevich. One major proof difficulty stems from high-degree vertices, which prevent maximal cliques in their neighborhoods: we deal with these vertices by an intricate union bound argument, that combines the probabilistic method with new degree counting arguments in order to enable Janson's inequality. This way we determine the asymptotics of the clique chromatic number of G_{n,p} in some ranges, and discover a surprising new phenomenon that contradicts earlier predictions for edge-probabilities p close to n^{-2/5}.","Tue, 5 Mar 2024 14:49:41 UTC (34 KB)"
"76","76","A Convex Optimization Framework for Computing Robustness Margins of Kalman Filters","Himanshu Prabhat, Raktim Bhattacharya","Systems and Control (eess.SY)","This paper proposes a novel convex optimization framework for designing robust Kalman filters that guarantee a user-specified steady-state error while maximizing process and sensor noise. The proposed framework simultaneously determines the Kalman gain and the robustness margin in terms of the process and sensor noise. This is the first paper to present such a joint formulation for Kalman filtering. The proposed methodology is validated through two distinct examples: the Clohessy-Wiltshire-Hill equations for a chaser spacecraft in an elliptical orbit and the longitudinal motion model of an F-16 aircraft.","Tue, 5 Mar 2024 14:21:58 UTC (343 KB)"
"77","77","Model Predictive Control for setpoint tracking","Daniel Limon, Antonio Ferramosca, Ignacio Alvarado, Teodoro Alamo","Optimization and Control (math.OC)","The main objective of tracking control is to steer the tracking error, that is the difference between the reference and the output, to zero while the plant's operation limits are satisfied. This requires that some assumptions on the evolution of the future values of the reference must be taken into account. Typically a simple evolution of the reference is considered, such as step, ramp, or parabolic reference signals. It is important to notice that the tracking problem considers possible variations in the reference to be tracked, such as steps or slope variations of the ramps. Then the tracking control problem is inherently uncertain, since the reference may differ from what is expected. If the value of the reference is changed, then there is no guarantee that the feasibility and stability properties of the resulting control law hold. This report presents the MPC for tracking (MPCT) approach, which ensures recursive feasibility and asymptotic stability of the setpoint when the value of the reference is changed.","Tue, 5 Mar 2024 13:52:48 UTC (269 KB)"
"78","78","Non-Convex Stochastic Composite Optimization with Polyak Momentum","Yuan Gao, Anton Rodomanov, Sebastian U. Stich","Optimization and Control (math.OC)","The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.","Tue, 5 Mar 2024 13:43:58 UTC (682 KB)"
"79","79","Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems","Tomás González, Cristóbal Guzmán, Courtney Paquette","Optimization and Control (math.OC)","We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(varepsilon, delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $sqrt{log(d)/n} + (log(d)^{3/2}/[nvarepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $sqrt{log(d)/n} + (log(d)^{3/2}/[nvarepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $log(d)/sqrt{n} + log(d)/[nvarepsilon]^{1/2}$ with constant success probability. This result provides evidence of the near-optimality of the approach. Finally, we show that combining our methods with acceleration techniques from online learning leads to the first algorithm for DP Stochastic Convex Optimization in the polyhedral setting that is not based on Frank-Wolfe methods. For convex and first-order-smooth stochastic objectives, our algorithms attain an excess risk of $sqrt{log(d)/n} + log(d)^{7/10}/[nvarepsilon]^{2/5}$, and when additionally assuming second-order-smoothness, we improve the rate to $sqrt{log(d)/n} + log(d)/sqrt{nvarepsilon}$. Instrumental to all of these results are various extensions of the classical Maurey Sparsification Lemma, which may be of independent interest.","Tue, 5 Mar 2024 12:28:00 UTC (97 KB)"
"80","80","A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails","Amit Attia, Tomer Koren","Machine Learning (cs.LG)","This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.","Tue, 5 Mar 2024 11:38:20 UTC (11 KB)"
"81","81","An Adaptive Hydropower Management Approach for Downstream Ecosystem Preservation","C. Coelho, M. Jing, M. Fernanda P. Costa, L.L. Ferrás","Machine Learning (cs.LG)","Hydropower plants play a pivotal role in advancing clean and sustainable energy production, contributing significantly to the global transition towards renewable energy sources. However, hydropower plants are currently perceived both positively as sources of renewable energy and negatively as disruptors of ecosystems. In this work, we highlight the overlooked potential of using hydropower plant as protectors of ecosystems by using adaptive ecological discharges. To advocate for this perspective, we propose using a neural network to predict the minimum ecological discharge value at each desired time. Additionally, we present a novel framework that seamlessly integrates it into hydropower management software, taking advantage of the well-established approach of using traditional constrained optimisation algorithms. This novel approach not only protects the ecosystems from climate change but also contributes to potentially increase the electricity production.","Tue, 5 Mar 2024 09:44:51 UTC (217 KB)"
"82","82","Linear quadratic control of nonlinear systems with Koopman operator learning and the Nyström method","Edoardo Caldarelli, Antoine Chatalic, Adrià Colomé, Cesare Molinari, Carlos Ocampo-Martinez, Carme Torras, Lorenzo Rosasco","Optimization and Control (math.OC)","In this paper, we study how the Koopman operator framework can be combined with kernel methods to effectively control nonlinear dynamical systems. While kernel methods have typically large computational requirements, we show how random subspaces (Nyström approximation) can be used to achieve huge computational savings while preserving accuracy. Our main technical contribution is deriving theoretical guarantees on the effect of the Nyström approximation. More precisely, we study the linear quadratic regulator problem, showing that both the approximated Riccati operator and the regulator objective, for the associated solution of the optimal control problem, converge at the rate $m^{-1/2}$, where $m$ is the random subspace size. Theoretical findings are complemented by numerical experiments corroborating our results.","Tue, 5 Mar 2024 09:28:40 UTC (933 KB)"
"83","83","Face-hitting Dominating Sets in Planar Graphs","P. Francis, Abraham M. Illickan, Lijo M. Jose, Deepak Rajendraprasad","Combinatorics (math.CO)","A dominating set of a graph $G$ is a subset $S$ of its vertices such that each vertex of $G$ not in $S$ has a neighbor in $S$. A face-hitting set of a plane graph $G$ is a set $T$ of vertices in $G$ such that every face of $G$ contains at least one vertex of $T$. We show that the vertex-set of every plane (multi-)graph without isolated vertices, self-loops or $2$-faces can be partitioned into two disjoint sets so that both the sets are dominating and face-hitting. We also show that all the three assumptions above are necessary for the conclusion. As a corollary, we show that every $n$-vertex simple plane triangulation has a dominating set of size at most $(1 - alpha)n/2$, where $alpha n$ is the maximum size of an independent set in the triangulation. Matheson and Tarjan [European J. Combin., 1996] conjectured that every plane triangulation with a sufficiently large number of vertices $n$ has a dominating set of size at most $n / 4$. Currently, the best known general bound for this is by Christiansen, Rotenberg and Rutschmann [SODA, 2024] who showed that every plane triangulation on $n > 10$ vertices has a dominating set of size at most $2n/7$. Our corollary improves their bound for $n$-vertex plane triangulations which contain a maximal independent set of size either less than $2n/7$ or more than $3n/7$.","Tue, 5 Mar 2024 09:24:11 UTC (15 KB)"
"84","84","Data Collaboration Analysis Over Matrix Manifolds","Keiyu Nosaka, Akiko Yoshise","Machine Learning (cs.LG)","The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of this data in developing robust ML models. Within this realm, the Non-Readily Identifiable Data Collaboration (NRI-DC) framework emerges as an innovative approach, potentially resolving the 'data island' issue among institutions through non-iterative communication and robust privacy protections. However, in its current state, the NRI-DC framework faces model performance instability due to theoretical unsteadiness in creating collaboration functions. This study establishes a rigorous theoretical foundation for these collaboration functions and introduces new formulations through optimization problems on matrix manifolds and efficient solutions. Empirical analyses demonstrate that the proposed approach, particularly the formulation over orthogonal matrix manifolds, significantly enhances performance, maintaining consistency and efficiency without compromising communication efficiency or privacy protections.","Tue, 5 Mar 2024 08:52:16 UTC (384 KB)"
"85","85","Neural Fractional Differential Equations","C. Coelho, M. Fernanda P. Costa, L.L. Ferrás","Machine Learning (cs.LG)","Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering. They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours. This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions. Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data. This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural ODE in modelling systems with memory or dependencies on past states, and it can effectively be applied to learn more intricate dynamical systems.","Tue, 5 Mar 2024 07:45:29 UTC (629 KB)"
"86","86","A Two-Stage Training Method for Modeling Constrained Systems With Neural Networks","C. Coelho, M. Fernanda P. Costa, L.L. Ferrás","Machine Learning (cs.LG)","Real-world systems are often formulated as constrained optimization problems. Techniques to incorporate constraints into Neural Networks (NN), such as Neural Ordinary Differential Equations (Neural ODEs), have been used. However, these introduce hyperparameters that require manual tuning through trial and error, raising doubts about the successful incorporation of constraints into the generated model. This paper describes in detail the two-stage training method for Neural ODEs, a simple, effective, and penalty parameter-free approach to model constrained systems. In this approach the constrained optimization problem is rewritten as two unconstrained sub-problems that are solved in two stages. The first stage aims at finding feasible NN parameters by minimizing a measure of constraints violation. The second stage aims to find the optimal NN parameters by minimizing the loss function while keeping inside the feasible region. We experimentally demonstrate that our method produces models that satisfy the constraints and also improves their predictive performance. Thus, ensuring compliance with critical system properties and also contributing to reducing data quantity requirements. Furthermore, we show that the proposed method improves the convergence to an optimal solution and improves the explainability of Neural ODE models. Our proposed two-stage training method can be used with any NN architectures.","Tue, 5 Mar 2024 07:37:47 UTC (836 KB)"
"87","87","SGD with Partial Hessian for Deep Neural Networks Optimization","Ying Sun, Hongwei Yong, Lei Zhang","Machine Learning (cs.LG)","Due to the effectiveness of second-order algorithms in solving classical optimization problems, designing second-order optimizers to train deep neural networks (DNNs) has attracted much research interest in recent years. However, because of the very high dimension of intermediate features in DNNs, it is difficult to directly compute and store the Hessian matrix for network optimization. Most of the previous second-order methods approximate the Hessian information imprecisely, resulting in unstable performance. In this work, we propose a compound optimizer, which is a combination of a second-order optimizer with a precise partial Hessian matrix for updating channel-wise parameters and the first-order stochastic gradient descent (SGD) optimizer for updating the other parameters. We show that the associated Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods. The proposed method, namely SGD with Partial Hessian (SGD-PH), inherits the advantages of both first-order and second-order optimizers. Compared with first-order optimizers, it adopts a certain amount of information from the Hessian matrix to assist optimization, while compared with the existing second-order optimizers, it keeps the good generalization performance of first-order optimizers. Experiments on image classification tasks demonstrate the effectiveness of our proposed optimizer SGD-PH. The code is publicly available at url{this https URL}.","Tue, 5 Mar 2024 06:10:21 UTC (175 KB)"
"88","88","On General Weighted Extropy of Extreme Ranked Set Sampling","Pradeep Kumar Sahu, Nitin Gupta","Statistics Theory (math.ST)","The extropy measure, introduced by Lad, Sanfilippo, and Agro in their (2015) paper in Statistical Science, has garnered significant interest over the past years. In this study, we present a novel representation for the weighted extropy within the context of extreme ranked set sampling. Additionally, we offer related findings such as stochastic orders, characterizations, and precise bounds. Our results shed light onthe comparison between the weighted extropy of extreme ranked set sampling and its counterpart in simple random sampling.","Tue, 5 Mar 2024 05:51:44 UTC (171 KB)"
"89","89","Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad","Sayantan Choudhury, Nazarii Tupitsa, Nicolas Loizou, Samuel Horvath, Martin Takac, Eduard Gorbunov","Machine Learning (cs.LG)","Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O left(frac{log T}{sqrt{T}} ight)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.","Tue, 5 Mar 2024 04:35:59 UTC (152 KB)"
"90","90","Privacy in Multi-agent Systems","Yongqiang Wang","Systems and Control (eess.SY)","With the increasing awareness of privacy and the deployment of legislations in various multi-agent system application domains such as power systems and intelligent transportation, the privacy protection problem for multi-agent systems is gaining increased traction in recent years. This article discusses some of the representative advancements in the filed.","Tue, 5 Mar 2024 03:40:39 UTC (996 KB)"
"91","91","Eight-Partitioning Points in 3D, and Efficiently Too","Boris Aronov, Abdul Basit, Gianluca Tasinato, Indu Ramesh, Uli Wagner","Computational Geometry (cs.CG)","An emph{eight-partition} of a finite set of points (respectively, of a continuous mass distribution) in $mathbb{R}^3$ consists of three planes that divide the space into $8$ octants, such that each open octant contains at most $1/8$ of the points (respectively, of the mass). In 1966, Hadwiger showed that any mass distribution in $mathbb{R}^3$ admits an eight-partition; moreover, one can prescribe the normal direction of one of the three planes. The analogous result for finite point sets follows by a standard limit argument. We prove the following variant of this result: Any mass distribution (or point set) in $mathbb{R}^3$ admits an eight-partition for which the intersection of two of the planes is a line with a prescribed direction. Moreover, we present an efficient algorithm for calculating an eight-partition of a set of $n$ points in~$mathbb{R}^3$ (with prescribed normal direction of one of the planes) in time $O^{*}(n^{5/2})$.","Tue, 5 Mar 2024 03:34:41 UTC (230 KB)"
"92","92","DNNLasso: Scalable Graph Learning for Matrix-Variate Data","Meixia Lin, Yangjing Zhang","Machine Learning (cs.LG)","We consider the problem of jointly learning row-wise and column-wise dependencies of matrix-variate observations, which are modelled separately by two precision matrices. Due to the complicated structure of Kronecker-product precision matrices in the commonly used matrix-variate Gaussian graphical models, a sparser Kronecker-sum structure was proposed recently based on the Cartesian product of graphs. However, existing methods for estimating Kronecker-sum structured precision matrices do not scale well to large scale datasets. In this paper, we introduce DNNLasso, a diagonally non-negative graphical lasso model for estimating the Kronecker-sum structured precision matrix, which outperforms the state-of-the-art methods by a large margin in both accuracy and computational time. Our code is available at this https URL.","Tue, 5 Mar 2024 02:49:00 UTC (3,142 KB)"
"93","93","MUSIC: Accelerated Convergence for Distributed Optimization With Inexact and Exact Methods","Mou Wu, Haibin Liao, Zhengtao Ding, Yonggang Xiao","Optimization and Control (math.OC)","Gradient-type distributed optimization methods have blossomed into one of the most important tools for solving a minimization learning task over a networked agent system. However, only one gradient update per iteration is difficult to achieve a substantive acceleration of convergence. In this paper, we propose an accelerated framework named as MUSIC allowing each agent to perform multiple local updates and a single combination in each iteration. More importantly, we equip inexact and exact distributed optimization methods into this framework, thereby developing two new algorithms that exhibit accelerated linear convergence and high communication efficiency. Our rigorous convergence analysis reveals the sources of steady-state errors arising from inexact policies and offers effective solutions. Numerical results based on synthetic and real datasets demonstrate both our theoretical motivations and analysis, as well as performance advantages.","Tue, 5 Mar 2024 02:02:00 UTC (644 KB)"
"94","94","Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces","Isao Ishikawa, Yuka Hashimoto, Masahiro Ikeda, Yoshinobu Kawahara","Dynamical Systems (math.DS)","This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and capturing them outside the original function space. Through the theory of rigged Hilbert space, our study provides a principled methodology to analyze the estimated spectrum and eigenfunctions of Koopman operators, and enables eigendecomposition within a rigged RKHS. We also propose a new effective method for reconstructing the dynamical system from temporally-sampled trajectory data of the dynamical system with solid theoretical guarantee. We conduct several numerical simulations using the van der Pol oscillator, the Duffing oscillator, the Hénon map, and the Lorenz attractor, and illustrate the performance of JetDMD with clear numerical computations of eigenvalues and accurate predictions of the dynamical systems.","Mon, 4 Mar 2024 22:28:20 UTC (7,870 KB)"
"95","95","Collision Avoidance and Geofencing for Fixed-wing Aircraft with Control Barrier Functions","Tamas G. Molnar, Suresh K. Kannan, James Cunningham, Kyle Dunlap, Kerianne L. Hobbs, Aaron D. Ames","Systems and Control (eess.SY)","Safety-critical failures often have fatal consequences in aerospace control. Control systems on aircraft, therefore, must ensure the strict satisfaction of safety constraints, preferably with formal guarantees of safe behavior. This paper establishes the safety-critical control of fixed-wing aircraft in collision avoidance and geofencing tasks. A control framework is developed wherein a run-time assurance (RTA) system modulates the nominal flight controller of the aircraft whenever necessary to prevent it from colliding with other aircraft or crossing a boundary (geofence) in space. The RTA is formulated as a safety filter using control barrier functions (CBFs) with formal guarantees of safe behavior. CBFs are constructed and compared for a nonlinear kinematic fixed-wing aircraft model. The proposed CBF-based controllers showcase the capability of safely executing simultaneous collision avoidance and geofencing, as demonstrated by simulations on the kinematic model and a high-fidelity dynamical model.","Mon, 4 Mar 2024 21:54:51 UTC (18,845 KB)[v2] Thu, 7 Mar 2024 01:01:38 UTC (1,704 KB)"
"96","96","A Simple Finite-Time Analysis of TD Learning with Linear Function Approximation","Aritra Mitra","Machine Learning (cs.LG)","We study the finite-time convergence of TD learning with linear function approximation under Markovian sampling. Existing proofs for this setting either assume a projection step in the algorithm to simplify the analysis, or require a fairly intricate argument to ensure stability of the iterates. We ask: 	extit{Is it possible to retain the simplicity of a projection-based analysis without actually performing a projection step in the algorithm?} Our main contribution is to show this is possible via a novel two-step argument. In the first step, we use induction to prove that under a standard choice of a constant step-size $alpha$, the iterates generated by TD learning remain uniformly bounded in expectation. In the second step, we establish a recursion that mimics the steady-state dynamics of TD learning up to a bounded perturbation on the order of $O(alpha^2)$ that captures the effect of Markovian sampling. Combining these pieces leads to an overall approach that considerably simplifies existing proofs. We conjecture that our inductive proof technique will find applications in the analyses of more complex stochastic approximation algorithms, and conclude by providing some examples of such applications.","Mon, 4 Mar 2024 20:40:02 UTC (22 KB)"
"97","97","Edge states for tight-binding operators with soft walls","Camilo Gómez Araya, David Gontier, Hanne Van Den Bosch","Mathematical Physics (math-ph)","We study one- and two-dimensional periodic tight-binding models under the presence of a potential that grows to infinity in one direction, hence preventing the particles to escape in this direction (the soft wall). We prove that a spectral flow appears in these corresponding edge models, as the wall is shifted. We identity this flow as a number of Bloch bands, and provide a lower bound for the number of edge states appearing in such models.","Mon, 4 Mar 2024 20:23:43 UTC (1,705 KB)"
"98","98","On the impact of measure pre-conditionings on general parametric ML models and transfer learning via domain adaptation","Joaquín Sánchez García","Machine Learning (stat.ML)","We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.","Mon, 4 Mar 2024 19:26:39 UTC (230 KB)"
"99","99","Statistical Query Lower Bounds for Learning Truncated Gaussians","Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis","Data Structures and Algorithms (cs.DS)","We study the problem of estimating the mean of an identity covariance Gaussian in the truncated setting, in the regime when the truncation set comes from a low-complexity family $mathcal{C}$ of sets. Specifically, for a fixed but unknown truncation set $S subseteq mathbb{R}^d$, we are given access to samples from the distribution $mathcal{N}(oldsymbol{ mu}, mathbf{ I})$ truncated to the set $S$. The goal is to estimate $oldsymbolmu$ within accuracy $epsilon>0$ in $ell_2$-norm. Our main result is a Statistical Query (SQ) lower bound suggesting a super-polynomial information-computation gap for this task. In more detail, we show that the complexity of any SQ algorithm for this problem is $d^{mathrm{poly}(1/epsilon)}$, even when the class $mathcal{C}$ is simple so that $mathrm{poly}(d/epsilon)$ samples information-theoretically suffice. Concretely, our SQ lower bound applies when $mathcal{C}$ is a union of a bounded number of rectangles whose VC dimension and Gaussian surface are small. As a corollary of our construction, it also follows that the complexity of the previously known algorithm for this task is qualitatively best possible.","Mon, 4 Mar 2024 18:30:33 UTC (43 KB)"
"100","100","Minimum acyclic number and maximum dichromatic number of oriented triangle-free graphs of a given order","Pierre Aboulker, Frédéric Havet, François Pirot, Juliette Schabanel","Combinatorics (math.CO)","Let $D$ be a digraph. Its acyclic number $vec{alpha}(D)$ is the maximum order of an acyclic induced subdigraph and its dichromatic number $vec{chi}(D)$ is the least integer $k$ such that $V(D)$ can be partitioned into $k$ subsets inducing acyclic subdigraphs. We study ${vec a}(n)$ and $vec t(n)$ which are the minimum of $vecalpha(D)$ and the maximum of $vec{chi}(D)$, respectively, over all oriented triangle-free graphs of order $n$. For every $epsilon>0$ and $n$ large enough, we show $(1/sqrt{2} - epsilon) sqrt{nlog n} leq vec{a}(n) leq frac{107}{8} sqrt n log n$ and $frac{8}{107} sqrt n/log n leq vec{t}(n) leq (sqrt 2 + epsilon) sqrt{n/log n}$. We also construct an oriented triangle-free graph on 25 vertices with dichromatic number~3, and show that every oriented triangle-free graph of order at most 17 has dichromatic number at most 2.","Mon, 4 Mar 2024 18:29:12 UTC (1,276 KB)"
"101","101","Koopman-Assisted Reinforcement Learning","Preston Rozwood, Edward Mehrez, Ludger Paehler, Wen Sun, Steven L. Brunton","Artificial Intelligence (cs.AI)","The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in reinforcement learning (RL) and control theory. However, these equations quickly become intractable for systems with high-dimensional states and nonlinearity. This paper explores the connection between the data-driven Koopman operator and Markov Decision Processes (MDPs), resulting in the development of two new RL algorithms to address these limitations. We leverage Koopman operator techniques to lift a nonlinear system into new coordinates where the dynamics become approximately linear, and where HJB-based methods are more tractable. In particular, the Koopman operator is able to capture the expectation of the time evolution of the value function of a given system via linear dynamics in the lifted coordinates. By parameterizing the Koopman operator with the control actions, we construct a ``Koopman tensor'' that facilitates the estimation of the optimal value function. Then, a transformation of Bellman's framework in terms of the Koopman tensor enables us to reformulate two max-entropy RL algorithms: soft value iteration and soft actor-critic (SAC). This highly flexible framework can be used for deterministic or stochastic systems as well as for discrete or continuous-time dynamics. Finally, we show that these Koopman Assisted Reinforcement Learning (KARL) algorithms attain state-of-the-art (SOTA) performance with respect to traditional neural network-based SAC and linear quadratic regulator (LQR) baselines on four controlled dynamical systems: a linear state-space system, the Lorenz system, fluid flow past a cylinder, and a double-well potential with non-isotropic stochastic forcing.","Mon, 4 Mar 2024 18:19:48 UTC (3,324 KB)"
"102","102","Graphical Quadratic Algebra","Dario Stein, Fabio Zanasi, Richard Samuelson, Robin Piedeleu","Logic in Computer Science (cs.LO)","We introduce Graphical Quadratic Algebra (GQA), a string diagrammatic calculus extending the language of Graphical Affine Algebra with a new generator characterised by invariance under rotation matrices. We show that GQA is a sound and complete axiomatisation for three different models: quadratic relations, which are a compositional formalism for least-squares problems, Gaussian stochastic processes, and Gaussian stochastic processes extended with non-determinisms. The equational theory of GQA sheds light on the connections between these perspectives, giving an algebraic interpretation to the interplay of stochastic behaviour, relational behaviour, non-determinism, and conditioning. As applications, we discuss various case studies, including linear regression, probabilistic programming, and electrical circuits with realistic (noisy) components.","Mon, 4 Mar 2024 18:14:19 UTC (401 KB)"
"103","103","Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling","Yu Huang, Zixin Wen, Yuejie Chi, Yingbin Liang","Machine Learning (cs.LG)","Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers. In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining. On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the interplay between the attention of feature-position and position-wise correlations.","Mon, 4 Mar 2024 17:24:03 UTC (616 KB)"
"104","104","Emergence of Multivariate Extremes in Multilayer Inhomogeneous Random Graphs","Daniel Cirkovic, Tiandong Wang, Daren B.H. Cline","Probability (math.PR)","In this paper, we propose a multilayer inhomogeneous random graph model (MIRG), whose layers may consist of both single-edge and multi-edge graphs. In the single layer case, it has been shown that the regular variation of the weight distribution underlying the inhomogeneous random graph implies the regular variation of the typical degree distribution. We extend this correspondence to the multilayer case by showing that the multivariate regular variation of the weight distribution implies the multivariate regular variation of the asymptotic degree distribution. Furthermore, in certain circumstances, the extremal dependence structure present in the weight distribution will be adopted by the asymptotic degree distribution. By considering the asymptotic degree distribution, a wider class of Chung-Lu and Norros-Reittu graphs may be incorporated into the MIRG layers. Additionally, we prove consistency of the Hill estimator when applied to degrees of the MIRG that have a tail index greater than 1. Simulation results indicate that, in practice, hidden regular variation may be consistently detected from an observed MIRG.","Mon, 4 Mar 2024 17:08:25 UTC (535 KB)"
"105","105","Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming","Yongquan Qu, Mohamed Aziz Bhouri, Pierre Gentine","Machine Learning (cs.LG)","Accurate representations of unknown and sub-grid physical processes through parameterizations (or closure) in numerical simulations with quantified uncertainty are critical for resolving the coarse-grained partial differential equations that govern many problems ranging from weather and climate prediction to turbulence simulations. Recent advances have seen machine learning (ML) increasingly applied to model these subgrid processes, resulting in the development of hybrid physics-ML models through the integration with numerical solvers. In this work, we introduce a novel framework for the joint estimation and uncertainty quantification of physical parameters and machine learning parameterizations in tandem, leveraging differentiable programming. Achieved through online training and efficient Bayesian inference within a high-dimensional parameter space, this approach is enabled by the capabilities of differentiable programming. This proof of concept underscores the substantial potential of differentiable programming in synergistically combining machine learning with differential equations, thereby enhancing the capabilities of hybrid physics-ML modeling.","Mon, 4 Mar 2024 17:02:23 UTC (556 KB)"
"106","106","On the arithmetic complexity of computing Gröbner bases of comaximal determinantal ideals","Sriram Gopalakrishnan","Symbolic Computation (cs.SC)","Let $M$ be an $n	imes n$ matrix of homogeneous linear forms over a field $Bbbk$. If the ideal $mathcal{I}_{n-2}(M)$ generated by minors of size $n-1$ is Cohen-Macaulay, then the Gulliksen-Negård complex is a free resolution of $mathcal{I}_{n-2}(M)$. It has recently been shown that by taking into account the syzygy modules for $mathcal{I}_{n-2}(M)$ which can be obtained from this complex, one can derive a refined signature-based Gröbner basis algorithm DetGB which avoids reductions to zero when computing a grevlex Gröbner basis for $mathcal{I}_{n-2}(M)$. In this paper, we establish sharp complexity bounds on DetGB. To accomplish this, we prove several results on the sizes of reduced grevlex Gröbner bases of reverse lexicographic ideals, thanks to which we obtain two main complexity results which rely on conjectures similar to that of Fröberg. The first one states that, in the zero-dimensional case, the size of the reduced grevlex Gröbner basis of $mathcal{I}_{n-2}(M)$ is bounded from below by $n^{6}$ asymptotically. The second, also in the zero-dimensional case, states that the complexity of DetGB is bounded from above by $n^{2omega+3}$ asymptotically, where $2leomegale 3$ is any complexity exponent for matrix multiplication over $Bbbk$.","Mon, 4 Mar 2024 16:08:33 UTC (39 KB)"
"107","107","On Efficient Approximation of the Maximum Distance to A Point Over an Intersection of Balls","Beniamin Costandin, Marius Costandin","Computational Geometry (cs.CG)","In this paper we study the NP-Hard problem of maximizing the distance over an intersection of balls to a given point. We expand the results found in cite{funcos1}, where the authors characterize the farthest in an intersection of balls $mathcal{Q}$ to the given point $C_0$ by constructing some intersection of halfspaces. In this paper, by slightly modifying the technique found in literature, we characterize the farthest in an intersection of balls $mathcal{Q}$ with another intersection of balls $mathcal{Q}_1$. As such, going backwards, we are naturally able to find the given intersection of balls $mathcal{Q}$ as the max indicator intersection of balls of another one $mathcal{Q}_{-1}$. By repeating the process, we find a sequence of intersection of balls $(mathcal{Q}_{i})_{i in mathbb{Z}}$, which has $mathcal{Q}$ as an element, namely $mathcal{Q}_{0}$ and show that $mathcal{Q}_{-infty} = mathcal{B}(C_0,R_0)$ where $R_0$ is the maximum distance from $C_0$ to a point in $mathcal{Q}$. As a final application of the proposed theory we give a polynomial algorithm for computing the maximum distance under an oracle which returns the volume of an intersection of balls, showing that the later is NP-Hard. Finally, we present a randomized method %of polynomial complexity which allows an approximation of the maximum distance.","Mon, 4 Mar 2024 14:19:55 UTC (1,495 KB)"
"108","108","Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations","Umut Şimşekli, Mert Gürbüzbalaban, Sinan Yıldırım, Lingjiong Zhu","Machine Learning (stat.ML)","Injecting heavy-tailed noise to the iterates of stochastic gradient descent (SGD) has received increasing attention over the past few years. While various theoretical properties of the resulting algorithm have been analyzed mainly from learning theory and optimization perspectives, their privacy preservation properties have not yet been established. Aiming to bridge this gap, we provide differential privacy (DP) guarantees for noisy SGD, when the injected noise follows an $alpha$-stable distribution, which includes a spectrum of heavy-tailed distributions (with infinite variance) as well as the Gaussian distribution. Considering the $(epsilon, delta)$-DP framework, we show that SGD with heavy-tailed perturbations achieves $(0, 	ilde{mathcal{O}}(1/n))$-DP for a broad class of loss functions which can be non-convex, where $n$ is the number of data points. As a remarkable byproduct, contrary to prior work that necessitates bounded sensitivity for the gradients or clipping the iterates, our theory reveals that under mild assumptions, such a projection step is not actually necessary. We illustrate that the heavy-tailed noising mechanism achieves similar DP guarantees compared to the Gaussian case, which suggests that it can be a viable alternative to its light-tailed counterparts.","Mon, 4 Mar 2024 13:53:41 UTC (63 KB)"
"109","109","Time-Reversal of Stochastic Maximum Principle","Amirhossein Taghvaei","Systems and Control (eess.SY)","Stochastic maximum principle (SMP) specifies a necessary condition for the solution of a stochastic optimal control problem. The condition involves a coupled system of forward and backward stochastic differential equations (FBSDE) for the state and the adjoint processes. Numerical solution of the FBSDE is challenging because the boundary condition of the adjoint process is specified at the terminal time, while the solution should be adaptable to the forward in time filtration of a Wiener process. In this paper, a ""time-reversal"" of the FBSDE system is proposed that involves integration with respect to a backward in time Wiener process. The time-reversal is used to propose an iterative Monte-Carlo procedure to solves the FBSDE system and its time-reversal simultaneously. The procedure involves approximating the {Föllmer's drift} and solving a regression problem between the state and its adjoint at each time. The procedure is illustrated for the linear quadratic (LQ) optimal control problem with a numerical example.","Mon, 4 Mar 2024 13:47:51 UTC (430 KB)"
"110","110","Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes with Point Singularities","Joost A. A. Opschoor, Christoph Schwab","Numerical Analysis (math.NA)","We analyze deep Neural Network emulation rates of smooth functions with point singularities in bounded, polytopal domains $mathrm{D} subset mathbb{R}^d$, $d=2,3$. We prove exponential emulation rates in Sobolev spaces in terms of the number of neurons and in terms of the number of nonzero coefficients for Gevrey-regular solution classes defined in terms of weighted Sobolev scales in $mathrm{D}$, comprising the countably-normed spaces of I.M. Babuška and B.Q. Guo. As intermediate result, we prove that continuous, piecewise polynomial high order (``$p$-version'') finite elements with elementwise polynomial degree $pinmathbb{N}$ on arbitrary, regular, simplicial partitions of polyhedral domains $mathrm{D} subset mathbb{R}^d$, $dgeq 2$ can be exactly emulated by neural networks combining ReLU and ReLU$^2$ activations. On shape-regular, simplicial partitions of polytopal domains $mathrm{D}$, both the number of neurons and the number of nonzero parameters are proportional to the number of degrees of freedom of the finite element space, in particular for the $hp$-Finite Element Method of I.M. Babuška and B.Q. Guo.","Mon, 4 Mar 2024 13:39:22 UTC (30 KB)"
"111","111","Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities","Rocco Caprio, Juan Kuntz, Samuel Power, Adam M. Johansen","Machine Learning (cs.LG)","We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--Łojasiewicz inequalities (LSI and PŁI, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the PŁI implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--Émery Theorem and show that the LSI/PŁI generalization holds for models with strongly concave log-likelihoods. For such models, we further control PGD's discretization error, obtaining non-asymptotic error bounds. While we are motivated by the study of PGD, we believe that the inequalities and results we extend may be of independent interest.","Mon, 4 Mar 2024 12:57:26 UTC (37 KB)"
"112","112","Characterization of Chordal Circular-arc Graphs: I. Split Graphs","Yixin Cao, Jan Derbisz, Tomasz Krawczyk","Combinatorics (math.CO)","The most elusive problem around the class of circular-arc graphs is identifying all minimal graphs that are not in this class. The main obstacle is the lack of a systematic way of enumerating these minimal graphs. McConnell [FOCS 2001] presented a transformation from circular-arc graphs to interval graphs with certain patterns of representations. We fully characterize these interval patterns for circular-arc graphs that are split graphs, thereby building a connection between minimal split graphs that are not circular-arc graphs and minimal non-interval graphs. This connection enables us to identify all minimal split graphs that are not circular-arc graphs. As a byproduct, we develop a linear-time certifying recognition algorithm for circular-arc graphs when the input is a split graph.","Mon, 4 Mar 2024 11:33:15 UTC (30 KB)"
"113","113","A Type Theory with a Tiny Object","Mitchell Riley","Category Theory (math.CT)","We present an extension of Martin-Löf Type Theory that contains a tiny object; a type for which there is a right adjoint to the formation of function types as well as the expected left adjoint. We demonstrate the practicality of this type theory by proving various properties related to tininess internally and suggest a few potential applications.","Mon, 4 Mar 2024 11:15:10 UTC (41 KB)"
"114","114","Capacity of the Hebbian-Hopfield network associative memory","Mihailo Stojnic","Machine Learning (stat.ML)","In cite{Hop82}, Hopfield introduced a emph{Hebbian} learning rule based neural network model and suggested how it can efficiently operate as an associative memory. Studying random binary patterns, he also uncovered that, if a small fraction of errors is tolerated in the stored patterns retrieval, the capacity of the network (maximal number of memorized patterns, $m$) scales linearly with each pattern's size, $n$. Moreover, he famously predicted $alpha_c=lim_{nightarrowinfty}frac{m}{n}approx 0.14$. We study this very same scenario with two famous pattern's basins of attraction: 	extbf{emph{(i)}} The AGS one from cite{AmiGutSom85}; and 	extbf{emph{(ii)}} The NLT one from cite{Newman88,Louk94,Louk94a,Louk97,Tal98}. Relying on the emph{fully lifted random duality theory} (fl RDT) from cite{Stojnicflrdt23}, we obtain the following explicit capacity characterizations on the first level of lifting: egin{equation} alpha_c^{(AGS,1)} = left ( max_{deltain left ( 0,frac{1}{2}ight ) }frac{1-2delta}{sqrt{2} mbox{erfinv} left ( 1-2deltaight )} - frac{2}{sqrt{2pi}} e^{-left ( mbox{erfinv}left ( 1-2delta ight )ight )^2}ight )^2 approx mathbf{0.137906} end{equation} egin{equation} alpha_c^{(NLT,1)} = frac{mbox{erf}(x)^2}{2x^2}-1+mbox{erf}(x)^2 approx mathbf{0.129490}, quad 1-mbox{erf}(x)^2- frac{2mbox{erf}(x)e^{-x^2}}{sqrt{pi}x}+frac{2e^{-2x^2}}{pi}=0. end{equation} A substantial numerical work gives on the second level of lifting $alpha_c^{(AGS,2)} approx mathbf{0.138186}$ and $alpha_c^{(NLT,2)} approx mathbf{0.12979}$, effectively uncovering a remarkably fast lifting convergence. Moreover, the obtained AGS characterizations exactly match the replica symmetry based ones of cite{AmiGutSom85} and the corresponding symmetry breaking ones of cite{SteKuh94}.","Mon, 4 Mar 2024 10:10:23 UTC (1,159 KB)"
"115","115","Online Locality Meets Distributed Quantum Computing","Amirreza Akbari, Xavier Coiteux-Roy, Francesco d'Amore, François Le Gall, Henrik Lievonen, Darya Melnyk, Augusto Modanese, Shreyas Pai, Marc-Olivier R","Distributed, Parallel, and Cluster Computing (cs.DC)","We extend the theory of locally checkable labeling problems (LCLs) from the classical LOCAL model to a number of other models that have been studied recently, including the quantum-LOCAL model, finitely-dependent processes, non-signaling model, dynamic-LOCAL model, and online-LOCAL model [e.g. STOC 2024, ICALP 2023]. First, we demonstrate the advantage that finitely-dependent processes have over the classical LOCAL model. We show that all LCL problems solvable with locality $O(log^* n)$ in the LOCAL model admit a finitely-dependent distribution (with constant locality). In particular, this gives a finitely-dependent coloring for regular trees, answering an open question by Holroyd [2023]. This also introduces a new formal barrier for understanding the distributed quantum advantage: it is not possible to exclude quantum advantage for any LCL in the $Theta(log^* n)$ complexity class by using non-signaling arguments. Second, we put limits on the capabilities of all of these models. To this end, we introduce a model called randomized online-LOCAL, which is strong enough to simulate e.g. SLOCAL and dynamic-LOCAL, and we show that it is also strong enough to simulate any non-signaling distribution and hence any quantum-LOCAL algorithm. We prove the following result for trees: if we can solve an LCL problem with locality $o(log^{(5)} n)$ in the randomized online-LOCAL model, we can solve it with locality $O(log^* n)$ in the classical deterministic LOCAL model. Put together, these results show that in trees the set of LCLs that can be solved with locality $O(log^* n)$ is the same across all these models: locality $O(log^* n)$ in quantum-LOCAL, non-signaling model, dynamic-LOCAL, or online-LOCAL is not stronger than locality $O(log^* n)$ in the classical deterministic LOCAL model.","Mon, 4 Mar 2024 10:03:54 UTC (1,634 KB)"
"116","116","Random Generation of Git Graphs","Julien Courtiel (GREYC), Martin Pépin (GREYC)","Data Structures and Algorithms (cs.DS)","Version Control Systems, such as Git and Mercurial, manage the history of a project as a Directed Acyclic Graph encoding the various divergences and synchronizations happening in its life cycle. A popular workflow in the industry, called the feature branch workflow, constrains these graphs to be of a particular shape: a unique main branch, and non-interfering feature branches. Here we focus on the uniform random generation of those graphs with n vertices, including k on the main branch, for which we provide three algorithms, for three different use-cases. The first, based on rejection, is efficient when aiming for small values of k (more precisely whenever k = O($sqrt$ n)). The second takes as input any number k of commits in the main branch, but requires costly precalculation. The last one is a Boltzmann generator and enables us to generate very large graphs while targeting a constant k/n ratio. All these algorithms are linear in the size of their outputs.","Mon, 4 Mar 2024 10:01:00 UTC (187 KB)"
"117","117","Information Lower Bounds for Robust Mean Estimation","Rémy Degenne, Timothée Mathieu","Statistics Theory (math.ST)","We prove lower bounds on the error of any estimator for the mean of a real probability distribution under the knowledge that the distribution belongs to a given set. We apply these lower bounds both to parametric and nonparametric estimation. In the nonparametric case, we apply our results to the question of sub-Gaussian estimation for distributions with finite variance to obtain new lower bounds in the small error probability regime, and present an optimal estimator in that regime. In the (semi-)parametric case, we use the Fisher information to provide distribution-dependent lower bounds that are constant-tight asymptotically, of order $sqrt{2log(1/delta)/(nI)}$ where $I$ is the Fisher information of the distribution. We use known minimizers of the Fisher information on some nonparametric set of distributions to give lower bounds in cases such as corrupted distributions, or bounded/semi-bounded distributions.","Mon, 4 Mar 2024 09:52:15 UTC (52 KB)"
"118","118","On $3$-dimensional MRD codes of type $langle x^{q^t},x+δx^{q^{2t}},G(x) angle$","Daniele Bartoli, Francesco Ghiandoni","Information Theory (cs.IT)","In this work we present results on the classification of $mathbb{F}_{q^n}$-linear MRD codes of dimension three. In particular, using connections with certain algebraic varieties over finite fields, we provide non-existence results for MRD codes $mathcal{C}=langle x^{q^t}, F(x), G(x) angle subseteq mathcal{L}_{n,q}$ of exceptional type, i.e. such that $mathcal{C}$ is MRD over infinite many extensions of the field $mathbb{F}_{q^n}$. These results partially address a conjecture of Bartoli, Zini and Zullo in 2023.","Mon, 4 Mar 2024 09:49:10 UTC (407 KB)"
"119","119","The positive energy theorem for weighted asymptotically anti-de Sitter spacetimes","Yaohua Wang, Xiao Zhang","General Relativity and Quantum Cosmology (gr-qc)","The positive energy theorem for weighted asymptotically flat spin manifolds was proved by Baldauf and Ozuch cite{BO}, and for non-spin case by Chu and Zhu cite{CZh}. In this paper, we generalize the positive energy theorem for 3-dimensional asymptotically anti-de Sitter initial data sets to weighted asymptotically anti-de Sitter initial data sets assuming that the weighted dominant energy condition holds.","Mon, 4 Mar 2024 09:42:05 UTC (14 KB)"
"120","120","Progressive Smoothing for Motion Planning in Real-Time NMPC","Rudolf Reiter, Katrin Baumgärtner, Rien Quirynen, Moritz Diehl","Systems and Control (eess.SY)","Nonlinear model predictive control (NMPC) is a popular strategy for solving motion planning problems, including obstacle avoidance constraints, in autonomous driving applications. Non-smooth obstacle shapes, such as rectangles, introduce additional local minima in the underlying optimization problem. Smooth over-approximations, e.g., ellipsoidal shapes, limit the performance due to their conservativeness. We propose to vary the smoothness and the related over-approximation by a homotopy. Instead of varying the smoothness in consecutive sequential quadratic programming iterations, we use formulations that decrease the smooth over-approximation from the end towards the beginning of the prediction horizon. Thus, the real-time iterations algorithm is applicable to the proposed NMPC formulation. Different formulations are compared in simulation experiments and shown to successfully improve performance indicators without increasing the computation time.","Mon, 4 Mar 2024 08:26:14 UTC (2,275 KB)"
"121","121","Macroscopic auxiliary asymptotic preserving neural networks for the linear radiative transfer equations","Hongyan Li, Song Jiang, Wenjun Sun, Liwei Xu, Guanyu Zhou","Numerical Analysis (math.NA)","We develop a Macroscopic Auxiliary Asymptotic-Preserving Neural Network (MA-APNN) method to solve the time-dependent linear radiative transfer equations (LRTEs), which have a multi-scale nature and high dimensionality. To achieve this, we utilize the Physics-Informed Neural Networks (PINNs) framework and design a new adaptive exponentially weighted Asymptotic-Preserving (AP) loss function, which incorporates the macroscopic auxiliary equation that is derived from the original transfer equation directly and explicitly contains the information of the diffusion limit equation. Thus, as the scale parameter tends to zero, the loss function gradually transitions from the transport state to the diffusion limit state. In addition, the initial data, boundary conditions, and conservation laws serve as the regularization terms for the loss. We present several numerical examples to demonstrate the effectiveness of MA-APNNs.","Mon, 4 Mar 2024 08:10:42 UTC (1,269 KB)"
"122","122","Tsallis Entropy Regularization for Linearly Solvable MDP and Linear Quadratic Regulator","Yota Hashizume, Koshi Oishi, Kenji Kashima","Optimization and Control (math.OC)","Shannon entropy regularization is widely adopted in optimal control due to its ability to promote exploration and enhance robustness, e.g., maximum entropy reinforcement learning known as Soft Actor-Critic. In this paper, Tsallis entropy, which is a one-parameter extension of Shannon entropy, is used for the regularization of linearly solvable MDP and linear quadratic regulators. We derive the solution for these problems and demonstrate its usefulness in balancing between exploration and sparsity of the obtained control law.","Mon, 4 Mar 2024 07:53:15 UTC (407 KB)"
"123","123","Weakly modular graphs with diamond condition, the interval function and axiomatic characterizations","Lekshmi Kamal Kamalolbhavan-Sheela, Jeny Jacob, Manoj Changat","Combinatorics (math.CO)","Weakly modular graphs are defined as the class of graphs that satisfy the emph{triangle condition ($TC$)} and the emph{quadrangle condition ($QC$)}. We study an interesting subclass of weakly modular graphs that satisfies a stronger version of the triangle condition, known as the emph{triangle diamond condition ($TDC$)}. and term this subclass of weakly modular graphs as the emph{diamond-weakly modular graphs}. It is observed that this class contains the class of bridged graphs and the class of weakly bridged graphs. The interval function $I_G$ of a connected graph $G$ with vertex set $V$ is an important concept in metric graph theory and is one of the prime example of a transit function; a set function defined on the Cartesian product $V	imes V$ to the power set of $V$ satisfying the expansive, symmetric and idempotent axioms. In this paper, we derive an interesting axiom denoted as $(J0')$, obtained from a well-known axiom introduced by Marlow Sholander in 1952, denoted as $(J0)$. It is proved that the axiom $(J0')$ is a characterizing axiom of the diamond-weakly modular graphs. We propose certain types of independent first-order betweenness axioms on an arbitrary transit function $R$ and prove that an arbitrary transit function becomes the interval function of a diamond-weakly modular graph if and only if $R$ satisfies these betweenness axioms. Similar characterizations are obtained for the interval function of bridged graphs and weakly bridged graphs.","Mon, 4 Mar 2024 06:59:03 UTC (491 KB)"
"124","124","A Safe Screening Rule with Bi-level Optimization of $ν$ Support Vector Machine","Zhiji Yang, Wanyi Chen, Huan Zhang, Yitian Xu, Lei Shi, Jianhua Zhao","Machine Learning (cs.LG)","Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the $
u$ support vector machine ($
u$-SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for $
u$-SVM (SRBO-$
u$-SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO-$
u$-SVM is strictly deduced by integrating the Karush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex problems and the $
u$-property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to accelerate many SVM-type models, and it is successfully applied to one-class SVM. Experimental results on 6 artificial data sets and 30 benchmark data sets have verified the effectiveness and safety of our proposed methods in supervised and unsupervised tasks.","Mon, 4 Mar 2024 06:55:57 UTC (2,675 KB)"
"125","125","How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems","Yuxiao Huang, Wenjie Zhang, Liang Feng, Xingyu Wu, Kay Chen Tan","Artificial Intelligence (cs.AI)","Recently, large language models (LLMs) have notably positioned them as capable tools for addressing complex optimization challenges. Despite this recognition, a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems. Keeping this in mind, we first propose to enhance the optimization performance using multimodal LLM capable of processing both textual and visual prompts for deeper insights of the processed optimization problem. This integration allows for a more comprehensive understanding of optimization problems, akin to human cognitive processes. We have developed a multimodal LLM-based optimization framework that simulates human problem-solving workflows, thereby offering a more nuanced and effective analysis. The efficacy of this method is evaluated through extensive empirical studies focused on a well-known combinatorial optimization problem, i.e., capacitated vehicle routing problem. The results are compared against those obtained from the LLM-based optimization algorithms that rely solely on textual prompts, demonstrating the significant advantages of our multimodal approach.","Mon, 4 Mar 2024 06:24:21 UTC (12,412 KB)"
"126","126","Statistical Mechanics of Dynamical System Identification","Andrei A. Klishin, Joseph Bakarji, J. Nathan Kutz, Krithika Manohar","Statistical Mechanics (cond-mat.stat-mech)","Recovering dynamical equations from observed noisy data is the central challenge of system identification. We develop a statistical mechanical approach to analyze sparse equation discovery algorithms, which typically balance data fit and parsimony through a trial-and-error selection of hyperparameters. In this framework, statistical mechanics offers tools to analyze the interplay between complexity and fitness, in analogy to that done between entropy and energy. To establish this analogy, we define the optimization procedure as a two-level Bayesian inference problem that separates variable selection from coefficient values and enables the computation of the posterior parameter distribution in closed form. A key advantage of employing statistical mechanical concepts, such as free energy and the partition function, is in the quantification of uncertainty, especially in in the low-data limit; frequently encountered in real-world applications. As the data volume increases, our approach mirrors the thermodynamic limit, leading to distinct sparsity- and noise-induced phase transitions that delineate correct from incorrect identification. This perspective of sparse equation discovery, is versatile and can be adapted to various other equation discovery algorithms.","Mon, 4 Mar 2024 04:32:28 UTC (2,038 KB)"
"127","127","Soft-constrained Schrodinger Bridge: a Stochastic Control Approach","Jhanvi Garg, Xianyang Zhang, Quan Zhou","Machine Learning (stat.ML)","Schrödinger bridge can be viewed as a continuous-time stochastic control problem where the goal is to find an optimally controlled diffusion process with a pre-specified terminal distribution $mu_T$. We propose to generalize this stochastic control problem by allowing the terminal distribution to differ from $mu_T$ but penalizing the Kullback-Leibler divergence between the two distributions. We call this new control problem soft-constrained Schrödinger bridge (SSB). The main contribution of this work is a theoretical derivation of the solution to SSB, which shows that the terminal distribution of the optimally controlled process is a geometric mixture of $mu_T$ and some other distribution. This result is further extended to a time series setting. One application of SSB is the development of robust generative diffusion models. We propose a score matching-based algorithm for sampling from geometric mixtures and showcase its use via a numerical example for the MNIST data set.","Mon, 4 Mar 2024 04:10:24 UTC (1,907 KB)"
"128","128","Atropos-k is PSPACE-complete","Chao Yang, Zhujun Zhang","Computational Complexity (cs.CC)","Burke and Teng introduced a two-player combinatorial game Atropos based on Sperner's lemma, and showed that deciding whether one has a winning strategy for Atropos is PSPACE-complete. In the original Atropos game, the players must color a node adjacent to the last colored node. Burke and Teng also mentioned a variant Atropos-k in which each move is at most of distance k of the previous move, and asked a question on determining the computational complexity of this variant. In this paper, we answer this question by showing that for any fixed integer k (k>=2), Atropos-k is PSPACE-complete by reduction from True Quantified Boolean Formula (TQBF).","Mon, 4 Mar 2024 01:07:41 UTC (31 KB)"
"129","129","Geometry and Stability of Supervised Learning Problems","Facundo Mémoli, Brantley Vose, Robert C. Williamson","Machine Learning (cs.LG)","We introduce a notion of distance between supervised learning problems, which we call the Risk distance. This optimal-transport-inspired distance facilitates stability results; one can quantify how seriously issues like sampling bias, noise, limited data, and approximations might change a given problem by bounding how much these modifications can move the problem under the Risk distance. With the distance established, we explore the geometry of the resulting space of supervised learning problems, providing explicit geodesics and proving that the set of classification problems is dense in a larger class of problems. We also provide two variants of the Risk distance: one that incorporates specified weights on a problem's predictors, and one that is more sensitive to the contours of a problem's risk landscape.","Mon, 4 Mar 2024 00:48:36 UTC (911 KB)"
"130","130","Noise sensitivity and stability on groups","Ryokichi Tanaka","Probability (math.PR)","We provide finitely generated infinite groups on which natural random walks are noise sensitive in total variation as well as ones on which natural random walks are noise stable in total variation.","Mon, 4 Mar 2024 00:44:07 UTC (37 KB)"
"131","131","Generalized pair-wise logit dynamic and its connection to a mean field game: theoretical and computational investigations focusing on resource managem","Hidekazu Yoshioka, Motoh Tsujimura","Optimization and Control (math.OC)","Logit dynamics are evolution equations that describe transitions to equilibria of actions among many players. We formulate a pair-wise logit dynamic in a continuous action space with a generalized exponential function, which we call a generalized pair-wise logit dynamic, depicted by a new evolution equation nonlocal in space. We prove the well-posedness and approximability of the generalized pair-wise logit dynamic to show that it is computationally implementable. We also show that this dynamic has an explicit connection to a mean field game of a controlled pure-jump process, with which the two different mathematical models can be understood in a unified way. Particularly, we show that the generalized pair-wise logit dynamic is derived as a myopic version of the corresponding mean field game, and that the conditions to guarantee the existence of unique solutions are different from each other. The key in this procedure is to find the objective function to be optimized in the mean field game based on the logit function. The monotonicity of the utility is unnecessary for the generalized pair-wise logit dynamic but crucial for the mean field game. Finally, we present applications of the two approaches to fisheries management problems with collected data.","Mon, 4 Mar 2024 00:41:34 UTC (2,233 KB)"
"132","132","Spectral antisymmetry of twisted graph adjacency","Ye Luo, Arindam Roy","Combinatorics (math.CO)","We address a prime counting problem across the homology classes of a graph, presenting a graph-theoretical Dirichlet-type analogue of the prime number theorem. The main machinery we have developed and employed is a spectral antisymmetry theorem, revealing that the spectra of the twisted graph adjacency matrices have an antisymmetric distribution over the character group of the graph. Additionally, we derive some trace formulas based on the twisted adjacency matrices as part of our analysis.","Sun, 3 Mar 2024 15:55:05 UTC (416 KB)"
"133","133","Local weak convergence and its applications","Sayan Banerjee, Shankar Bhamidi, Jianan Shen, Seth Parker Young","Probability (math.PR)","Motivated in part by understanding average case analysis of fundamental algorithms in computer science, and in part by the wide array of network data available over the last decade, a variety of random graph models, with corresponding processes on these objects, have been proposed over the last few years. The main goal of this paper is to give an overview of local weak convergence, which has emerged as a major technique for understanding large network asymptotics for a wide array of functionals and models. As opposed to a survey, the main goal is to try to explain some of the major concepts and their use to junior researchers in the field and indicate potential resources for further reading.","Sun, 3 Mar 2024 15:43:19 UTC (9,601 KB)"
"134","134","Conditional normality and finite-state dimensions revisited","Alexander Shen","Information Theory (cs.IT)","The notion of a normal bit sequence was introduced by Borel in 1909; it was the first definition of an individual random object. Normality is a weak notion of randomness requiring only that all $2^n$ factors (substrings) of arbitrary length~$n$ appear with the same limit frequency $2^{-n}$. Later many stronger definitions of randomness were introduced, and in this context normality found its place as ``randomness against a finite-memory adversary''. A quantitative measure of finite-state compressibility was also introduced (the finite-state dimension) and normality means that the finite state dimension is maximal (equals~$1$). Recently Nandakumar, Pulari and S (2023) introduced the notion of relative finite-state dimension for a binary sequence with respect to some other binary sequence (treated as an oracle), and the corresponding notion of conditional (relative) normality. (Different notions of conditional randomness were considered before, but not for the finite memory case.) They establish equivalence between the block frequency and the gambling approaches to conditional normality and finite-state dimensions. In this note we revisit their definitions and explain how this equivalence can be obtained easily by generalizing known characterizations of (unconditional) normality and dimension in terms of compressibility (finite-state complexity), superadditive complexity measures and gambling (finite-state gales), thus also answering some questions left open in the above-mentioned paper.","Sun, 3 Mar 2024 15:23:56 UTC (19 KB)"
"135","135","A new family of $2$-scattered subspaces and related MRD codes","Daniele Bartoli, Francesco Ghiandoni, Alessandro Giannoni, Giuseppe Marino","Combinatorics (math.CO)","Scattered subspaces and $h$-scattered subspaces have been extensively studied in recent decades for both theoretical purposes and their connections to various applications. While numerous constructions of scattered subspaces exist, relatively few are known about $h$-scattered subspaces with $hgeq2$. In this paper, we establish the existence of maximum $2$-scattered $F_q$-subspaces in $V(r,q^6)$ whenever $rgeq 3$, $r
e 5$, and $q$ is an odd power of $2$. Additionally, we explore the corresponding MRD codes.","Sun, 3 Mar 2024 13:09:20 UTC (22 KB)"
"136","136","A face-centred finite volume method for laminar and turbulent incompressible flows","Luan M. Vieira, Matteo Giacomini, Ruben Sevilla, Antonio Huerta","Fluid Dynamics (physics.flu-dyn)","This work develops, for the first time, a face-centred finite volume (FCFV) solver for the simulation of laminar and turbulent viscous incompressible flows. The formulation relies on the Reynolds-averaged Navier-Stokes (RANS) equations coupled with the negative Spalart-Allmaras (SA) model and three novel convective stabilisations, inspired by Riemann solvers, are derived and compared numerically. The resulting method achieves first-order convergence of the velocity, the velocity-gradient tensor and the pressure. FCFV accurately predicts engineering quantities of interest, such as drag and lift, on unstructured meshes and, by avoiding gradient reconstruction, the method is insensitive to mesh quality, even in the presence of highly distorted and stretched cells. A monolithic and a staggered solution strategies for the RANS-SA system are derived and compared numerically. Numerical benchmarks, involving laminar and turbulent, steady and transient cases are used to assess the performance, accuracy and robustness of the proposed FCFV method.","Sun, 3 Mar 2024 12:12:47 UTC (17,916 KB)"
"137","137","Ultimate codes","Ted Hurley","Information Theory (cs.IT)","A linear block code over a field can be derived from a unit scheme. Looking at codes as structures within a unit scheme greatly extends the availability of linear block and convolutional codes and allows the construction of the codes to required length, rate, distance and type. Properties of a code emanate from properties of the unit from which it was derived. %% can thus be constructed and analysed by designating the units whose properties would give the required codes. Orthogonal units, units in group rings, Fourier/Vandermonde units and related units are used to construct and analyse linear block and convolutional codes and to construct these to predefined length, rate, distance and type. Self-dual, dual containing, quantum error-correcting and complementary dual linear block and convolutional codes are constructed. Low density parity check linear block and convolutional codes are constructed using group rings and are constructed with no short cycles in the control matrix. From a single unit, multiple codes of a required type are derivable.","Sun, 3 Mar 2024 12:02:46 UTC (41 KB)[v2] Tue, 5 Mar 2024 14:59:00 UTC (41 KB)"
"138","138","Distributed Discrete-time Dynamic Outer Approximation of the Intersection of Ellipsoids","Eduardo Sebastián, Rodrigo Aldana-López, Rosario Aragüés, Eduardo Montijano, Carlos Sagüés","Optimization and Control (math.OC)","This paper presents the first discrete-time distributed algorithm to track the tightest ellipsoids that outer approximates the global dynamic intersection of ellipsoids. The ellipsoids are defined as time-varying positive definite matrices. On the other hand, given an undirected network, each node is equipped with one of these ellipsoids. The solution is based on a novel distributed reformulation of the original centralized semi-definite outer Löwner-John program, characterized by a non-separable objective function and global constraints. We prove finite-time convergence to the global minima of the centralized problem in the static case and finite-time bounded tracking error in the dynamic case. Moreover, we prove boundedness of estimation in the tracking of the global optimum and robustness in the estimation against time-varying inputs. As a by-product, the proposed algorithm extends min/max dynamic consensus algorithms to positive definite matrices. We illustrate the properties of the algorithm with different simulated examples, including a distributed estimation showcase where our proposal is integrated into a distributed Kalman filter to surpass the state-of-the-art in mean square error performance.","Sun, 3 Mar 2024 11:08:52 UTC (1,614 KB)"
"139","139","Preserving correlations: A statistical method for generating synthetic data","Nicklas Jävergård, Rainey Lyons, Adrian Muntean, Jonas Forsman","Machine Learning (cs.LG)","We propose a method to generate statistically representative synthetic data. The main goal is to be able to maintain in the synthetic dataset the correlations of the features present in the original one, while offering a comfortable privacy level that can be eventually tailored on specific customer demands. We describe in detail our algorithm used both for the analysis of the original dataset and for the generation of the synthetic data points. The approach is tested using a large energy-related dataset. We obtain good results both qualitatively (e.g. via vizualizing correlation maps) and quantitatively (in terms of suitable $ell^1$-type error norms used as evaluation metrics). The proposed methodology is general in the sense that it does not rely on the used test dataset. We expect it to be applicable in a much broader context than indicated here.","Sun, 3 Mar 2024 10:35:46 UTC (493 KB)"
"140","140","Distributed Least-Squares Optimization Solvers with Differential Privacy","Weijia Liu, Lei Wang, Fanghong Guo, Zhengguang Wu, Hongye Su","Optimization and Control (math.OC)","This paper studies the distributed least-squares optimization problem with differential privacy requirement of local cost functions, for which two differentially private distributed solvers are proposed. The first is established on the distributed gradient tracking algorithm, by appropriately perturbing the initial values and parameters that contain the privacy-sensitive data with Gaussian and truncated Laplacian noises, respectively. Rigorous proofs are established to show the achievable trade-off between the ({epsilon}, {delta})-differential privacy and the computation accuracy. The second solver is established on the combination of the distributed shuffling mechanism and the average consensus algorithm, which enables each agent to obtain a noisy version of parameters characterizing the global gradient. As a result, the least-squares optimization problem can be eventually solved by each agent locally in such a way that any given ({epsilon}, {delta})-differential privacy requirement can be preserved while the solution may be computed with the accuracy independent of the network size, which makes the latter more suitable for large-scale distributed least-squares problems. Numerical simulations are presented to show the effectiveness of both solvers.","Sun, 3 Mar 2024 08:14:50 UTC (92 KB)"
"141","141","The Implicit Bias of Heterogeneity towards Invariance and Causality","Yang Xu, Yihong Gu, Cong Fang","Machine Learning (cs.LG)","It is observed empirically that the large language models (LLM), trained with a variant of regression loss using numerous corpus from the Internet, can unveil causal associations to some extent. This is contrary to the traditional wisdom that ``association is not causation'' and the paradigm of traditional causal inference in which prior causal knowledge should be carefully incorporated into the design of methods. It is a mystery why causality, in a higher layer of understanding, can emerge from the regression task that pursues associations. In this paper, we claim the emergence of causality from association-oriented training can be attributed to the coupling effects from the heterogeneity of the source data, stochasticity of training algorithms, and over-parameterization of the learning models. We illustrate such an intuition using a simple but insightful model that learns invariance, a quasi-causality, using regression loss. To be specific, we consider multi-environment low-rank matrix sensing problems where the unknown r-rank ground-truth d*d matrices diverge across the environments but contain a lower-rank invariant, causal part. In this case, running pooled gradient descent will result in biased solutions that only learn associations in general. We show that running large-batch Stochastic Gradient Descent, whose each batch being linear measurement samples randomly selected from a certain environment, can successfully drive the solution towards the invariant, causal solution under certain conditions. This step is related to the relatively strong heterogeneity of the environments, the large step size and noises in the optimization algorithm, and the over-parameterization of the model. In summary, we unveil another implicit bias that is a result of the symbiosis between the heterogeneity of data and modern algorithms, which is, to the best of our knowledge, first in the literature.","Sun, 3 Mar 2024 07:38:24 UTC (105 KB)"
"142","142","Uniform $mathcal{C}^k$ Approximation of $G$-Invariant and Antisymmetric Functions, Embedding Dimensions, and Polynomial Representations","Soumya Ganguly, Khoa Tran, Rahul Sarkar","Machine Learning (cs.LG)","For any subgroup $G$ of the symmetric group $mathcal{S}_n$ on $n$ symbols, we present results for the uniform $mathcal{C}^k$ approximation of $G$-invariant functions by $G$-invariant polynomials. For the case of totally symmetric functions ($G = mathcal{S}_n$), we show that this gives rise to the sum-decomposition Deep Sets ansatz of Zaheer et al. (2018), where both the inner and outer functions can be chosen to be smooth, and moreover, the inner function can be chosen to be independent of the target function being approximated. In particular, we show that the embedding dimension required is independent of the regularity of the target function, the accuracy of the desired approximation, as well as $k$. Next, we show that a similar procedure allows us to obtain a uniform $mathcal{C}^k$ approximation of antisymmetric functions as a sum of $K$ terms, where each term is a product of a smooth totally symmetric function and a smooth antisymmetric homogeneous polynomial of degree at most $inom{n}{2}$. We also provide upper and lower bounds on $K$ and show that $K$ is independent of the regularity of the target function, the desired approximation accuracy, and $k$.","Sat, 2 Mar 2024 23:19:10 UTC (73 KB)"
"143","143","Quantifying Maximum Actuator Degradation for a Given $H_2/H_{infty}$ Performance with Full-State Feedback Control","Hrishav Das, Eliot Nychka, Raktim Bhattacharya","Systems and Control (eess.SY)","In this paper, we address the issue of quantifying maximum actuator degradation in linear time-invariant dynamical systems. We present a new unified framework for computing the state-feedback controller gain that meets a user-defined closed-loop performance criterion while also maximizing actuator degradation. This degradation is modeled as a first-order filter with additive noise. Our approach involves two novel convex optimization formulations that concurrently determine the controller gain, maximize actuator degradation, and maintain the desired closed-loop performance in both the $H_2$ and $H_{infty}$ system norms. The results are limited to open-loop stable systems. We demonstrate the application of our results through the design of a full-state feedback controller for a model representing the longitudinal motion of the F-16 aircraft.","Sat, 2 Mar 2024 22:38:44 UTC (968 KB)"
"144","144","The legacy of Bletchley Park on UK mathematics","Daniel Shiu","History and Overview (math.HO)","The second world war saw a major influx of mathematical talent into the areas of cryptanalysis and cryptography. This was particularly true at the UK's Government Codes and Cypher School (GCCS) at Bletchley Park. The success of introducing mathematical thinking into activities previously dominated by linguists is well-studied, but the reciprocal question of how the cryptologic effort affected the field of mathematics has been less investigated. Although their cryptologic achievements are not as celebrated as those of Turing, Tutte and Welchman, Bletchley Park's effort was supplemented by more eminent mathematicians, and those who would achieve eminence and provide leadership and direction for mathematical research in the United Kingdom. Amongst their number were Ian Cassels, Sandy Green, Philip Hall, Max Newman and Henry Whitehead. This paper considers how the experience of these and other mathematicians at Bletchley Park may have informed and influenced the mathematics that was produced in their post-war careers.","Sat, 2 Mar 2024 22:32:49 UTC (15 KB)"
"145","145","Network analysis using Krylov subspace trajectories","H. Robert Frost","Physics and Society (physics.soc-ph)","We describe a set of network analysis methods based on the rows of the Krylov subspace matrix computed from a network adjacency matrix via power iteration using a non-random initial vector. We refer to these node-specific row vectors as Krylov subspace trajectories. While power iteration using a random initial starting vector is commonly applied to the network adjacency matrix to compute eigenvector centrality values, this application only uses the final vector generated after numerical convergence. Importantly, use of a random initial vector means that the intermediate results of power iteration are also random and lack a clear interpretation. To the best of our knowledge, use of intermediate power iteration results for network analysis has been limited to techniques that leverage just a single pre-convergence solution, e.g., Power Iteration Clustering. In this paper, we explore methods that apply power iteration with a non-random inital vector to the network adjacency matrix to generate Krylov subspace trajectories for each node. These non-random trajectories provide important information regarding network structure, node importance, and response to perturbations. We have created this short preprint in part to generate feedback from others in the network analysis community who might be aware of similar existing work.","Sat, 2 Mar 2024 17:13:00 UTC (408 KB)"
"146","146","Decentralized Implicit Differentiation","Lucas Fuentes Valenzuela, Robin Brown, Marco Pavone","Optimization and Control (math.OC)","The ability to differentiate through optimization problems has unlocked numerous applications, from optimization-based layers in machine learning models to complex design problems formulated as bilevel programs. It has been shown that exploiting problem structure can yield significant computation gains for optimization and, in some cases, enable distributed computation. One should expect that this structure can be similarly exploited for gradient computation. In this work, we discuss a decentralized framework for computing gradients of constraint-coupled optimization problems. First, we show that this framework results in significant computational gains, especially for large systems, and provide sufficient conditions for its validity. Second, we leverage exponential decay of sensitivities in graph-structured problems towards building a fully distributed algorithm with convergence guarantees. Finally, we use the methodology to rigorously estimate marginal emissions rates in power systems models. Specifically, we demonstrate how the distributed scheme allows for accurate and efficient estimation of these important emissions metrics on large dynamic power system models.","Sat, 2 Mar 2024 16:39:38 UTC (2,402 KB)"
"147","147","Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise","Halyun Jeong, Deanna Needell, Elizaveta Rebrova","Machine Learning (cs.LG)","We propose SGD-exp, a stochastic gradient descent approach for linear and ReLU regressions under Massart noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence guarantees of SGD-exp to the true parameter with up to $50%$ Massart corruption rate, and with any corruption rate in the case of symmetric oblivious corruptions. This is the first convergence guarantee result for robust ReLU regression in the streaming setting, and it shows the improved convergence rate over previous robust methods for $L_1$ linear regression due to a choice of an exponentially decaying step size, known for its efficiency in practice. Our analysis is based on the drift analysis of a discrete stochastic process, which could also be interesting on its own.","Sat, 2 Mar 2024 12:45:01 UTC (1,136 KB)"
"148","148","A Composite Decomposition Method for Large-Scale Global Optimization","Maojiang Tian, Minyang Chen, Wei Du, Yang Tang, Yaochu Jin, Gary G. Yen","Optimization and Control (math.OC)","Cooperative co-evolution (CC) algorithms, based on the divide-and-conquer strategy, have emerged as the predominant approach to solving large-scale global optimization (LSGO) problems. The efficiency and accuracy of the grouping stage significantly impact the performance of the optimization process. While the general separability grouping (GSG) method has overcome the limitation of previous differential grouping (DG) methods by enabling the decomposition of non-additively separable functions, it suffers from high computational complexity. To address this challenge, this article proposes a composite separability grouping (CSG) method, seamlessly integrating DG and GSG into a problem decomposition framework to utilize the strengths of both approaches. CSG introduces a step-by-step decomposition framework that accurately decomposes various problem types using fewer computational resources. By sequentially identifying additively, multiplicatively and generally separable variables, CSG progressively groups non-separable variables by recursively considering the interactions between each non-separable variable and the formed non-separable groups. Furthermore, to enhance the efficiency and accuracy of CSG, we introduce two innovative methods: a multiplicatively separable variable detection method and a non-separable variable grouping method. These two methods are designed to effectively detect multiplicatively separable variables and efficiently group non-separable variables, respectively. Extensive experimental results demonstrate that CSG achieves more accurate variable grouping with lower computational complexity compared to GSG and state-of-the-art DG series designs.","Sat, 2 Mar 2024 12:12:04 UTC (315 KB)"
"149","149","Edge-guided Low-light Image Enhancement with Inertial Bregman Alternating Linearized Minimization","Chaoyan Huang, Zhongming Wu, Tieyong Zeng","Computer Vision and Pattern Recognition (cs.CV)","Prior-based methods for low-light image enhancement often face challenges in extracting available prior information from dim images. To overcome this limitation, we introduce a simple yet effective Retinex model with the proposed edge extraction prior. More specifically, we design an edge extraction network to capture the fine edge features from the low-light image directly. Building upon the Retinex theory, we decompose the low-light image into its illumination and reflectance components and introduce an edge-guided Retinex model for enhancing low-light images. To solve the proposed model, we propose a novel inertial Bregman alternating linearized minimization algorithm. This algorithm addresses the optimization problem associated with the edge-guided Retinex model, enabling effective enhancement of low-light images. Through rigorous theoretical analysis, we establish the convergence properties of the algorithm. Besides, we prove that the proposed algorithm converges to a stationary point of the problem through nonconvex optimization theory. Furthermore, extensive experiments are conducted on multiple real-world low-light image datasets to demonstrate the efficiency and superiority of the proposed scheme.","Sat, 2 Mar 2024 09:00:57 UTC (41,634 KB)"
"150","150","LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation","Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, Yue-Jiao Gong","Optimization and Control (math.OC)","Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior optimization performance compared to GPT-4 Turbo and the other competitors across both synthetic and realistic problem sets. The fine-tuned model and the usage instructions are available at https://anonymous.4open.science/r/LLaMoCo-722A.","Sat, 2 Mar 2024 08:21:59 UTC (4,006 KB)[v2] Tue, 5 Mar 2024 11:11:41 UTC (4,006 KB)"
